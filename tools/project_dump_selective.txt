PROJECT STRUCTURE:
==================================================

./
  convert_all_xlsx_to_csv.py
  analyze_text_files.py
  create_simple_dump.py
  deep_analyze.py


FILE CONTENTS:
==================================================

==============================
FILE: convert_all_xlsx_to_csv.py
==============================

#!/usr/bin/env python3
"""
tools/convert_all_xlsx_to_csv.py
--------------------------------
Convert all Excel files (.xlsx, .xls) in a directory tree to CSV format.

Features:
  - Handles multiple sheets by creating separate CSV per sheet.
  - Optionally deletes original Excel files after conversion.
Usage:
  python tools/convert_all_xlsx_to_csv.py [directory]
      [--exclude DIR [DIR ...]] [--delete-original] [--yes] [--encoding ENCODING]
"""

import argparse
import logging
import os
from pathlib import Path
from typing import List, Optional

import pandas as pd

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)

class ExcelToCSVConverter:
    def __init__(
        self,
        exclude_dirs: Optional[List[str]] = None,
        delete_original: bool = False,
        encoding: str = 'utf-8'
    ) -> None:
        """
        Args:
            exclude_dirs: directories to skip (e.g. ['.git', '__pycache__'])
            delete_original: delete Excel files after conversion
            encoding: encoding for output CSV files
        """
        self.exclude_dirs = exclude_dirs or [
            '.git', '.venv', '.venv312', '__pycache__',
            'node_modules', 'dist', 'build', '.idea'
        ]
        self.delete_original = delete_original
        self.encoding = encoding

        # Statistics
        self.files_found = 0
        self.files_converted = 0
        self.errors: List[str] = []
        self.total_size_before = 0
        self.total_size_after = 0

    def should_skip_directory(self, path: Path) -> bool:
        """Return True if any part of `path` matches an excluded directory."""
        return any(part in self.exclude_dirs for part in path.parts)

    def find_excel_files(self, root: Path) -> List[Path]:
        """Recursively find all .xlsx and .xls files under `root`."""
        excel_files: List[Path] = []
        logger.info("Scanning %s for Excel files…", root)
        for dirpath, dirnames, filenames in os.walk(root):
            current = Path(dirpath)
            if self.should_skip_directory(current):
                dirnames.clear()  # don't recurse into excluded dirs
                continue
            for fn in filenames:
                if fn.lower().endswith(('.xlsx', '.xls')):
                    file_path = current / fn
                    excel_files.append(file_path)
                    self.total_size_before += file_path.stat().st_size
                    logger.debug("Found Excel: %s", file_path.relative_to(root))
        self.files_found = len(excel_files)
        logger.info("Total Excel files found: %d", self.files_found)
        return excel_files

    def convert_single_file(self, excel_path: Path) -> bool:
        """
        Convert one Excel file to CSV. Returns True on success.
        Creates one CSV if single sheet, otherwise one per sheet.
        """
        try:
            logger.info("Converting %s", excel_path)
            if not excel_path.exists():
                raise FileNotFoundError(f"{excel_path} not found")

            xls = pd.ExcelFile(excel_path)
            sheets = xls.sheet_names

            if len(sheets) == 1:
                df = pd.read_excel(xls, sheets[0])
                csv_path = excel_path.with_suffix('.csv')
                df.to_csv(csv_path, index=False, encoding=self.encoding)
                self.total_size_after += csv_path.stat().st_size
                logger.info("  → %s (%d rows × %d cols)", csv_path.name, len(df), len(df.columns))
            else:
                logger.info("  Multiple sheets: %s", sheets)
                for sheet in sheets:
                    df = pd.read_excel(xls, sheet_name=sheet)
                    csv_name = f"{excel_path.stem}_{sheet}.csv"
                    csv_path = excel_path.parent / csv_name
                    df.to_csv(csv_path, index=False, encoding=self.encoding)
                    self.total_size_after += csv_path.stat().st_size
                    logger.info("    → %s (%d rows × %d cols)", csv_name, len(df), len(df.columns))

            if self.delete_original:
                excel_path.unlink()
                logger.info("  Deleted original: %s", excel_path.name)

            self.files_converted += 1
            return True

        except Exception as e:
            err = f"Failed to convert {excel_path}: {e}"
            self.errors.append(err)
            logger.error(err)
            return False

    def convert_all(self, root: Path, confirm: bool = True) -> None:
        """
        Orchestrate find → convert → summary.
        If `confirm` is True, prompts the user before proceeding.
        """
        excel_files = self.find_excel_files(root)
        if not excel_files:
            logger.warning("No Excel files to convert under %s", root)
            return

        logger.info("About to convert %d file(s)", len(excel_files))
        if self.delete_original:
            logger.warning("Original Excel files will be deleted after conversion")
        if confirm:
            resp = input("Proceed with conversion? (y/n): ").strip().lower()
            if resp != 'y':
                logger.info("Conversion cancelled by user")
                return

        for f in excel_files:
            self.convert_single_file(f)

        self.print_summary()

    def print_summary(self) -> None:
        """Log conversion statistics and space savings."""
        logger.info("Conversion complete: %d/%d files succeeded, %d errors",
                    self.files_converted, self.files_found, len(self.errors))
        if self.total_size_before:
            saved = self.total_size_before - self.total_size_after
            pct = saved / self.total_size_before * 100
            logger.info("Space before: %s, after: %s (%.1f%% saved)",
                        self.format_bytes(self.total_size_before),
                        self.format_bytes(self.total_size_after),
                        pct)
        if self.errors:
            logger.error("Errors encountered:")
            for e in self.errors:
                logger.error("  - %s", e)

    @staticmethod
    def format_bytes(size: int) -> str:
        """Convert `size` in bytes to human-readable string."""
        for unit in ('B', 'KB', 'MB', 'GB'):
            if size < 1024:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} TB"

def main() -> None:
    """Parse arguments and run the converter."""
    parser = argparse.ArgumentParser(
        description="Convert all Excel files in a directory tree to CSV."
    )
    parser.add_argument(
        "directory",
        nargs="?",
        default=".",
        help="Root directory to scan (default: current directory)"
    )
    parser.add_argument(
        "--exclude",
        nargs="+",
        help="Additional directories to skip"
    )
    parser.add_argument(
        "--delete-original",
        action="store_true",
        help="Delete Excel files after successful conversion"
    )
    parser.add_argument(
        "--yes",
        action="store_true",
        help="Do not prompt for confirmation"
    )
    parser.add_argument(
        "--encoding",
        default="utf-8",
        help="Encoding for output CSV files (default: utf-8)"
    )

    args = parser.parse_args()
    converter = ExcelToCSVConverter(
        exclude_dirs=args.exclude,
        delete_original=args.delete_original,
        encoding=args.encoding
    )
    root_path = Path(args.directory)
    converter.convert_all(root_path, confirm=not args.yes)


if __name__ == "__main__":
    main()

==============================
FILE: analyze_text_files.py
==============================

#!/usr/bin/env python3
"""
tools/analyze_text_files.py
---------------------------
Generate a selective project dump including only code files,
and produce a list of excluded large files.

Usage:
    python tools/analyze_text_files.py [--dump-output DUMP_FILE] [--excluded-output EXCLUDED_FILE]
"""
import os
import logging
from argparse import ArgumentParser
from pathlib import Path
from typing import Set, Tuple

# Configure logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

def create_selective_dump(
    output_file: Path,
    code_extensions: Tuple[str, ...],
    exclude_dirs: Set[str],
    max_file_size: int
) -> None:
    """
    Write a dump of project structure and contents limited to code files under max size.
    """
    included_count = 0
    excluded_count = 0

    with output_file.open("w", encoding="utf-8") as f:
        # 1. Project structure
        f.write("PROJECT STRUCTURE:\n")
        f.write("=" * 50 + "\n\n")
        for root, dirs, files in os.walk("."):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            level = root.replace(".", "").count(os.sep)
            indent = "  " * level
            dir_name = os.path.basename(root) or Path(root).resolve().name
            f.write(f"{indent}{dir_name}/\n")
            sub_indent = "  " * (level + 1)
            for file in files:
                if file.endswith(code_extensions):
                    f.write(f"{sub_indent}{file}\n")

        # 2. File contents
        f.write("\n\nFILE CONTENTS:\n")
        f.write("=" * 50 + "\n\n")
        for root, dirs, files in os.walk("."):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            for file in files:
                if file.endswith(code_extensions):
                    path = Path(root) / file
                    rel = path.relative_to(Path("."))
                    try:
                        size = path.stat().st_size
                        if size > max_file_size:
                            excluded_count += 1
                            continue
                        f.write("=" * 30 + "\n")
                        f.write(f"FILE: {rel}\n")
                        f.write("=" * 30 + "\n\n")
                        f.write(path.read_text(encoding="utf-8"))
                        included_count += 1
                    except Exception as e:
                        f.write(f"Error reading {rel}: {e}\n")
                    f.write("\n")

    logger.info(
        "Created dump '%s' (included %d files, excluded %d files)",
        output_file, included_count, excluded_count
    )

def list_excluded_files(
    output_file: Path,
    exclude_dirs: Set[str],
    max_size: int,
    report_extensions: Tuple[str, ...]
) -> None:
    """
    Generate a list of non-code files exceeding max_size.
    """
    errors = []
    with output_file.open("w", encoding="utf-8") as f:
        f.write("Large files excluded from dump:\n\n")
        for root, dirs, files in os.walk("."):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            for file in files:
                if file.endswith(report_extensions):
                    path = Path(root) / file
                    try:
                        size = path.stat().st_size
                        if size > max_size:
                            rel = path.relative_to(Path("."))
                            f.write(f"{rel}: {size // 1024}KB\n")
                    except Exception as e:
                        errors.append(f"{path}: {e}")
        if errors:
            f.write("\nErrors:\n")
            for err in errors:
                f.write(f" - {err}\n")
    logger.info("Created excluded-files list '%s'", output_file)

def main() -> None:
    parser = ArgumentParser(
        description="Selective dump of code files and list of excluded large files"
    )
    parser.add_argument(
        "--dump-output", "-d",
        type=Path,
        default=Path("project_dump_selective.txt"),
        help="Filename for the selective dump"
    )
    parser.add_argument(
        "--excluded-output", "-e",
        type=Path,
        default=Path("excluded_files.txt"),
        help="Filename for the excluded-files list"
    )
    parser.add_argument(
        "--max-size", "-m",
        type=int,
        default=500 * 1024,
        help="Maximum file size in bytes to include in the dump"
    )

    args = parser.parse_args()

    code_exts = (".py", ".yml", ".yaml", ".toml", ".cfg", ".ini")
    exclude_dirs = {"venv", ".venv312", "__pycache__", ".git", ".idea", "build", "dist"}
    report_exts = (".json", ".csv", ".txt", ".md")

    create_selective_dump(args.dump_output, code_exts, exclude_dirs, args.max_size)
    list_excluded_files(args.excluded_output, exclude_dirs, args.max_size, report_exts)

if __name__ == "__main__":
    main()

==============================
FILE: create_simple_dump.py
==============================

#!/usr/bin/env python3
"""
tools/create_simple_dump.py
---------------------------
Generate a simple project dump listing and embedding all Python source files
from specified directories.

Usage:
    python tools/create_simple_dump.py [--targets DIR [DIR ...]] [--output OUTPUT]

Defaults:
    targets: sponsor_match, scripts, app, .
    output : project_dump_simple.txt
"""

import argparse
import logging
import os
from pathlib import Path
from typing import List

# Configure structured logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

def collect_python_files(directories: List[Path]) -> List[Path]:
    """
    Walk each directory in `directories`, skipping __pycache__,
    and return a list of all .py file paths.
    """
    py_files: List[Path] = []
    for d in directories:
        if not d.exists():
            logger.warning("Skipping missing target directory: %s", d)
            continue

        if d.is_file() and d.suffix == ".py":
            py_files.append(d)
            continue

        for root, dirs, files in os.walk(d):
            # Skip __pycache__ directories
            dirs[:] = [dn for dn in dirs if dn != "__pycache__"]
            for fn in files:
                if fn.endswith(".py"):
                    py_files.append(Path(root) / fn)

    logger.info("Collected %d Python files", len(py_files))
    return py_files

def write_dump(files: List[Path], out_path: Path) -> None:
    """
    Write the header, file listing, and file contents to `out_path`.
    """
    header = "SPONSORMATCHAI PROJECT DUMP\n" + "="*50 + "\n\n"
    with out_path.open("w", encoding="utf-8") as f:
        f.write(header)

        # File listing
        f.write("FILES INCLUDED:\n" + "-"*40 + "\n")
        for p in files:
            f.write(f"{p}\n")
        f.write("\n")

        # File contents
        for p in files:
            f.write("="*40 + "\n")
            f.write(f"FILE: {p}\n")
            f.write("="*40 + "\n\n")
            try:
                content = p.read_text(encoding="utf-8")
                f.write(content)
            except Exception as e:
                f.write(f"<Error reading file: {e}>\n")
            f.write("\n\n")

    size_kb = out_path.stat().st_size / 1024
    logger.info("Created %s (%.1f KB)", out_path, size_kb)

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Create a simple dump of Python source files."
    )
    parser.add_argument(
        "--targets",
        nargs="+",
        type=Path,
        default=[Path(p) for p in ("sponsor_match", "scripts", "app", ".")],
        help="Directories (or files) to include"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("project_dump_simple.txt"),
        help="Output dump filename"
    )
    args = parser.parse_args()

    python_files = collect_python_files(args.targets)
    write_dump(python_files, args.output)

if __name__ == "__main__":
    main()

==============================
FILE: deep_analyze.py
==============================

#!/usr/bin/env python3
"""
tools/deep_analyze.py
---------------------
Analyze the project directory to report:
  1. File counts and total size by top-level directory
  2. File counts and total size by extension
  3. Suspicious “library” directories with many Python files

Usage:
    python tools/deep_analyze.py [--root DIR] [--top-n N]
"""

import os
import logging
from argparse import ArgumentParser
from pathlib import Path
from collections import defaultdict
from typing import DefaultDict, Tuple

# Configure logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

def analyze_project_structure(
    root_dir: Path,
    code_extensions: Tuple[str, ...],
    lib_indicators: Tuple[str, ...],
    top_n_dirs: int
) -> None:
    """
    Walk `root_dir` and compute:
      - Number and total size of code files per top-level subdirectory
      - Number and total size of code files per file extension
      - Directories indicating embedded libraries/modules
    """
    dir_counts: DefaultDict[str, int] = defaultdict(int)
    ext_counts: DefaultDict[str, int] = defaultdict(int)
    total_size_by_dir: DefaultDict[str, int] = defaultdict(int)
    total_size_by_ext: DefaultDict[str, int] = defaultdict(int)

    logger.info("Starting analysis in %s", root_dir)

    # Traverse and collect stats
    for current, dirs, files in os.walk(root_dir):
        # Determine the top-level directory relative to root_dir
        rel = Path(current).resolve().relative_to(root_dir.resolve())
        top_dir = rel.parts[0] if rel.parts else "."

        for fname in files:
            if not any(fname.endswith(ext) for ext in code_extensions):
                continue
            fpath = Path(current) / fname
            try:
                size = fpath.stat().st_size
            except OSError:
                logger.debug("Could not stat %s", fpath)
                continue

            ext = fpath.suffix
            dir_counts[top_dir] += 1
            ext_counts[ext] += 1
            total_size_by_dir[top_dir] += size
            total_size_by_ext[ext] += size

    # Report top-level directories
    logger.info("Files by top-level directory (showing top %d):", top_n_dirs)
    for dir_name, count in sorted(dir_counts.items(), key=lambda x: x[1], reverse=True)[:top_n_dirs]:
        size_mb = total_size_by_dir[dir_name] / (1024 * 1024)
        logger.info("  %-20s %6d files  %6.1f MB", dir_name, count, size_mb)

    # Report by extension
    logger.info("Files by extension:")
    for ext, count in sorted(ext_counts.items(), key=lambda x: x[1], reverse=True):
        size_mb = total_size_by_ext[ext] / (1024 * 1024)
        logger.info("  %-6s %6d files  %6.1f MB", ext or "<none>", count, size_mb)

    # Identify “suspicious” directories
    logger.info("Suspicious directories (likely vendor/lib code):")
    for current, dirs, files in os.walk(root_dir):
        for indicator in lib_indicators:
            if indicator in current:
                py_files = sum(1 for f in files if f.endswith(".py"))
                if py_files > 10:
                    rel_path = Path(current).resolve().relative_to(root_dir.resolve())
                    logger.info("  %-30s %3d Python files", rel_path, py_files)
                break

def main() -> None:
    parser = ArgumentParser(
        description="Deep analysis of project structure and code distribution"
    )
    parser.add_argument(
        "--root",
        type=Path,
        default=Path("."),
        help="Project root directory to analyze (default: current directory)"
    )
    parser.add_argument(
        "--top-n",
        type=int,
        default=15,
        help="How many top directories to display by file count"
    )
    args = parser.parse_args()

    code_exts: Tuple[str, ...] = (
        ".py", ".yml", ".yaml", ".toml", ".cfg", ".ini"
    )
    lib_inds: Tuple[str, ...] = (
        "site-packages", "lib", "Lib", "include", "Include",
        "__pycache__", "dist-packages", "node_modules",
        ".eggs", "build", "dist"
    )

    analyze_project_structure(args.root, code_exts, lib_inds, args.top_n)

if __name__ == "__main__":
    main()

