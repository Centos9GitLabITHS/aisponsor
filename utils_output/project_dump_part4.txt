================================================================================
FIL: sponsor_match/core/logger.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
sponsor_match/core/logger.py
----------------------------
Utility to configure and retrieve named loggers, with console
and optional file handlers.
"""

import logging
import sys
from pathlib import Path

def setup_logger(
    name: str,
    log_file: Path | None = None,
    level: int = logging.INFO
) -> logging.Logger:
    """
    Return a logger configured with:
      - StreamHandler (stdout) at `level`
      - Optional FileHandler if `log_file` is provided
    """
    logger = logging.getLogger(name)
    if logger.handlers:
        # Already configured
        return logger

    logger.setLevel(level)
    fmt = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    formatter = logging.Formatter(fmt)

    console = logging.StreamHandler(sys.stdout)
    console.setLevel(level)
    console.setFormatter(formatter)
    logger.addHandler(console)

    if log_file:
        file_h = logging.FileHandler(log_file)
        file_h.setLevel(level)
        file_h.setFormatter(formatter)
        logger.addHandler(file_h)

    return logger


================================================================================
FIL: sponsor_match/ml/__init__.py
================================================================================



================================================================================
FIL: sponsor_match/ml/pipeline.py
================================================================================

#!/usr/bin/env python3
"""
sponsor_match/ml/pipeline.py

Complete ML Pipeline with geocoded data support, proper scoring normalization,
and enhanced clustering capabilities. This replaces the existing pipeline.py
with full support for the new geocoded CSV files.
"""

import logging
import math
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Tuple, Optional

import joblib
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

from sponsor_match.core.db import get_engine

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Model paths
MODELS_DIR = Path(__file__).resolve().parents[2] / "models"
DEFAULT_KMEANS = MODELS_DIR / "kmeans.joblib"
LARGE_KMEANS = MODELS_DIR / "kmeans_large.joblib"

# Data paths for geocoded files
DATA_DIR = Path(__file__).resolve().parents[2] / "data"
ASSOCIATIONS_GEOCODED = DATA_DIR / "associations_geocoded.csv"
COMPANIES_GEOCODED = DATA_DIR / "companies_geocoded.csv"


@dataclass
class ScoringWeights:
    """Weights for different scoring components. Must sum to 1.0."""
    distance: float = 0.4
    size_match: float = 0.3
    cluster_match: float = 0.2
    industry_affinity: float = 0.1

    def __post_init__(self):
        """Validate that weights sum to 1.0."""
        total = self.distance + self.size_match + self.cluster_match + self.industry_affinity
        if abs(total - 1.0) > 1e-6:
            raise ValueError(f"Weights must sum to 1.0, got {total}")


def haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """
    Compute great-circle distance (km) between two points.

    This function calculates the shortest distance between two points on Earth's
    surface, accounting for the spherical nature of the planet.
    """
    R = 6371.0  # Earth's radius in kilometers

    # Convert degrees to radians
    phi1, lam1, phi2, lam2 = map(math.radians, (lat1, lon1, lat2, lon2))

    # Haversine formula
    dphi = phi2 - phi1
    dlam = lam2 - lam1
    a = (math.sin(dphi / 2) ** 2 +
         math.cos(phi1) * math.cos(phi2) * math.sin(dlam / 2) ** 2)

    return 2 * R * math.asin(math.sqrt(a))


def size_bucket_to_numeric(size_bucket: str) -> int:
    """Convert size bucket to numeric value for calculations."""
    mapping = {"small": 0, "medium": 1, "large": 2}
    return mapping.get(size_bucket, 1)


def validate_coordinates(lat: float, lon: float, entity_name: str = "") -> bool:
    """
    Validate that coordinates are within valid ranges.

    Returns True if valid, logs warning and returns False if invalid.
    """
    if lat is None or lon is None:
        logger.warning(f"Missing coordinates for {entity_name}")
        return False

    if not (-90 <= lat <= 90):
        logger.warning(f"Invalid latitude {lat} for {entity_name}")
        return False

    if not (-180 <= lon <= 180):
        logger.warning(f"Invalid longitude {lon} for {entity_name}")
        return False

    return True


def load_geocoded_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Load geocoded CSV files with validation and fallback to database if needed.

    Returns tuple of (associations_df, companies_df) with validated coordinates.
    """
    try:
        # Try loading geocoded CSV files first
        if ASSOCIATIONS_GEOCODED.exists() and COMPANIES_GEOCODED.exists():
            logger.info("Loading geocoded CSV files...")

            # Load associations
            associations_df = pd.read_csv(ASSOCIATIONS_GEOCODED)
            # Rename columns if needed for consistency
            if 'lat' in associations_df.columns:
                associations_df = associations_df.rename(columns={'lat': 'latitude', 'lon': 'longitude'})

            # Load companies
            companies_df = pd.read_csv(COMPANIES_GEOCODED)
            if 'lat' in companies_df.columns:
                companies_df = companies_df.rename(columns={'lat': 'latitude', 'lon': 'longitude'})

            # Validate and filter
            valid_assoc = associations_df.apply(
                lambda row: validate_coordinates(row['latitude'], row['longitude'], row['name']),
                axis=1
            )
            valid_comp = companies_df.apply(
                lambda row: validate_coordinates(row['latitude'], row['longitude'], row['name']),
                axis=1
            )

            associations_df = associations_df[valid_assoc]
            companies_df = companies_df[valid_comp]

            logger.info(
                f"Loaded {len(associations_df)} associations and {len(companies_df)} companies from geocoded CSVs")
            return associations_df, companies_df

    except Exception as e:
        logger.warning(f"Failed to load geocoded CSVs: {e}. Falling back to database...")

    # Fallback to database
    engine = get_engine()
    with engine.connect() as conn:
        associations_df = pd.read_sql(
            "SELECT * FROM associations WHERE lat IS NOT NULL AND lon IS NOT NULL",
            conn
        )
        companies_df = pd.read_sql(
            "SELECT * FROM companies WHERE lat IS NOT NULL AND lon IS NOT NULL",
            conn
        )

    return associations_df, companies_df


def load_models() -> Optional[Dict[str, any]]:
    """Load clustering models with enhanced error handling."""
    try:
        models = {}

        if DEFAULT_KMEANS.exists():
            model_data = joblib.load(DEFAULT_KMEANS)
            # Handle both old format (direct model) and new format (dict with scaler)
            if isinstance(model_data, dict):
                models["default"] = model_data
            else:
                # Legacy format - wrap in dict
                models["default"] = {
                    'kmeans': model_data,
                    'scaler': StandardScaler()  # Create default scaler
                }

        if LARGE_KMEANS.exists():
            model_data = joblib.load(LARGE_KMEANS)
            if isinstance(model_data, dict):
                models["large"] = model_data
            else:
                models["large"] = {
                    'kmeans': model_data,
                    'scaler': StandardScaler()
                }

        return models if models else None

    except Exception as e:
        logger.error(f"Failed to load models: {e}")
        return None


def calculate_distance_score(distance_km: float, max_distance: float = 50.0) -> float:
    """
    Calculate normalized distance score (1.0 = very close, 0.0 = far away).

    Uses exponential decay to prioritize closer matches.
    """
    if distance_km >= max_distance:
        return 0.0

    # Exponential decay with configurable rate
    decay_rate = 2.0  # Adjust to control how quickly score drops with distance
    return math.exp(-decay_rate * distance_km / max_distance)


def calculate_size_match_score(assoc_size: str, comp_size: str) -> float:
    """
    Calculate size compatibility score.

    Perfect match = 1.0, adjacent size = 0.5, opposite ends = 0.0
    """
    size_map = {"small": 0, "medium": 1, "large": 2}

    assoc_val = size_map.get(assoc_size, 1)
    comp_val = size_map.get(comp_size, 1)

    diff = abs(assoc_val - comp_val)

    if diff == 0:
        return 1.0
    elif diff == 1:
        return 0.5
    else:
        return 0.0


def calculate_industry_affinity(association: pd.Series, company: pd.Series) -> float:
    """
    Calculate industry-association affinity score.

    This is a simplified version - in production, you'd want a more sophisticated
    mapping based on historical sponsorship data.
    """
    # Simple rule-based affinities
    affinities = {
        ('sports', 'retail'): 0.8,
        ('sports', 'finance'): 0.7,
        ('sports', 'technology'): 0.6,
        ('cultural', 'finance'): 0.9,
        ('cultural', 'retail'): 0.7,
        ('youth', 'education'): 0.9,
        ('youth', 'technology'): 0.8,
    }

    # Extract categories (simplified - you'd parse from actual data)
    assoc_type = 'sports'  # Default for this implementation
    comp_industry = company.get('industry', 'other').lower()

    return affinities.get((assoc_type, comp_industry), 0.5)


def predict_cluster_safe(model_data: Dict, features: List[float]) -> Optional[int]:
    """
    Safely predict cluster with proper feature handling and validation.
    """
    try:
        if 'scaler' in model_data and 'kmeans' in model_data:
            # Scale features
            features_scaled = model_data['scaler'].transform([features[:2]])  # Use only lat/lon
            # Predict cluster
            return model_data['kmeans'].predict(features_scaled)[0]
        elif hasattr(model_data, 'predict'):
            # Legacy format
            return model_data.predict([features[:2]])[0]
    except Exception as e:
        logger.debug(f"Cluster prediction failed: {e}")
        return None


def score_and_rank(
        association_id: int,
        bucket: str,
        max_distance: float = 50.0,
        top_n: int = 10,
        weights: Optional[ScoringWeights] = None
) -> List[Dict]:
    """
    Score and rank potential sponsors with proper normalization and validation.

    This is the main function that orchestrates the matching process:
    1. Loads association and company data
    2. Calculates multiple scoring components
    3. Combines scores with validated weights
    4. Returns top N matches with normalized scores
    """
    if weights is None:
        weights = ScoringWeights()

    # Load data (prefer geocoded CSVs)
    associations_df, companies_df = load_geocoded_data()

    # Find the target association
    if 'id' in associations_df.columns:
        assoc_mask = associations_df['id'] == association_id
    else:
        # Fallback for CSV data without ID column
        assoc_mask = associations_df.index == association_id

    if not assoc_mask.any():
        logger.warning(f"Association {association_id} not found")
        return []

    association = associations_df[assoc_mask].iloc[0]

    # Get association coordinates
    assoc_lat = association.get('latitude', association.get('lat'))
    assoc_lon = association.get('longitude', association.get('lon'))

    if not validate_coordinates(assoc_lat, assoc_lon, f"Association {association_id}"):
        return []

    # Load models for clustering
    models = load_models()
    assoc_cluster = None

    if models:
        model_key = "large" if bucket == "large" else "default"
        model = models.get(model_key)

        if model:
            assoc_features = [assoc_lat, assoc_lon]
            assoc_cluster = predict_cluster_safe(model, assoc_features)

    # Score each company
    recommendations = []

    for _, company in companies_df.iterrows():
        try:
            # Get company coordinates
            comp_lat = company.get('latitude', company.get('lat'))
            comp_lon = company.get('longitude', company.get('lon'))

            if not validate_coordinates(comp_lat, comp_lon, company.get('name', 'Unknown')):
                continue

            # Calculate distance
            distance_km = haversine(assoc_lat, assoc_lon, comp_lat, comp_lon)

            # Skip if beyond max distance
            if distance_km > max_distance:
                continue

            # Calculate component scores
            distance_score = calculate_distance_score(distance_km, max_distance)
            size_score = calculate_size_match_score(
                association.get('size_bucket', bucket),
                company.get('size_bucket', 'medium')
            )

            # Cluster matching score
            cluster_score = 0.5  # Default neutral score
            if models and assoc_cluster is not None:
                model_key = "large" if company.get('size_bucket') == "large" else "default"
                model = models.get(model_key)
                if model:
                    comp_features = [comp_lat, comp_lon]
                    comp_cluster = predict_cluster_safe(model, comp_features)
                    if comp_cluster is not None and comp_cluster == assoc_cluster:
                        cluster_score = 1.0

            # Industry affinity
            industry_score = calculate_industry_affinity(association, company)

            # Combine scores with validated weights
            final_score = (
                    weights.distance * distance_score +
                    weights.size_match * size_score +
                    weights.cluster_match * cluster_score +
                    weights.industry_affinity * industry_score
            )

            # Ensure score is in valid range [0, 1]
            final_score = np.clip(final_score, 0.0, 1.0)

            recommendations.append({
                "id": int(company.get('id', 0)),
                "name": str(company.get('name', 'Unknown Company')),
                "lat": float(comp_lat),
                "lon": float(comp_lon),
                "distance": round(distance_km, 2),
                "score": round(final_score, 4),
                "components": {
                    "distance_score": round(distance_score, 3),
                    "size_score": round(size_score, 3),
                    "cluster_score": round(cluster_score, 3),
                    "industry_score": round(industry_score, 3)
                }
            })

        except Exception as e:
            logger.error(f"Error processing company {company.get('name', 'unknown')}: {e}")
            continue

    # Sort by score and return top N
    recommendations.sort(key=lambda x: x["score"], reverse=True)
    return recommendations[:top_n]


def recalibrate_models():
    """
    Recalibrate clustering models using geocoded data.

    This function should be run periodically to update models with new data.
    """
    logger.info("Recalibrating models with geocoded data...")

    # Load geocoded data
    associations_df, companies_df = load_geocoded_data()

    # Combine all entities for clustering
    all_coords = pd.concat([
        associations_df[['latitude', 'longitude', 'size_bucket']].rename(
            columns={'latitude': 'lat', 'longitude': 'lon'}
        ),
        companies_df[['latitude', 'longitude', 'size_bucket']].rename(
            columns={'latitude': 'lat', 'longitude': 'lon'}
        )
    ], ignore_index=True)

    # Train separate models for different size buckets
    from sklearn.cluster import KMeans

    for bucket, model_path in [("default", DEFAULT_KMEANS), ("large", LARGE_KMEANS)]:
        if bucket == "default":
            data = all_coords[all_coords['size_bucket'].isin(['small', 'medium'])]
        else:
            data = all_coords[all_coords['size_bucket'] == 'large']

        if len(data) < 5:
            logger.warning(f"Insufficient data for {bucket} model")
            continue

        # Prepare features
        features = data[['lat', 'lon']].values

        # Scale features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # Train KMeans
        n_clusters = min(10, len(data) // 5)  # Adaptive cluster count
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        kmeans.fit(features_scaled)

        # Save model with scaler
        model_data = {
            'kmeans': kmeans,
            'scaler': scaler,
            'n_features': 2,
            'n_clusters': n_clusters
        }

        joblib.dump(model_data, model_path)
        logger.info(f"Saved {bucket} model with {n_clusters} clusters")


# Main entry point for testing
if __name__ == "__main__":
    # Test the pipeline
    results = score_and_rank(
        association_id=1,
        bucket="medium",
        max_distance=50,
        top_n=5
    )

    print(f"Found {len(results)} recommendations:")
    for i, rec in enumerate(results, 1):
        print(f"{i}. {rec['name']} - Score: {rec['score'] * 100:.1f}% - Distance: {rec['distance']}km")

================================================================================
FIL: sponsor_match/models/__init__.py
================================================================================



================================================================================
FIL: sponsor_match/models/club_extended.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
models/club_extended.py
------------------------
Extended data model for sports clubs, including enrichment fields
for membership, financials, and sponsorship details.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional


@dataclass
class ExtendedClub:
    """
    Represents a sports club with both basic and enriched metadata.
    """

    # Basic info
    id: int
    name: str
    member_count: int
    address: str
    lat: Optional[float]
    lon: Optional[float]
    size_bucket: str

    # Extended basic info
    founded_year: int
    club_type: str
    registration_number: str
    website: str
    email: str
    phone: str
    social_media: Dict[str, str] = field(default_factory=dict)

    # Sports & activities
    sport_types: List[str] = field(default_factory=list)
    primary_sport: str = ""
    leagues: List[str] = field(default_factory=list)
    division_level: int = 0

    # Membership breakdown
    active_members: int = 0
    youth_members: int = 0
    gender_distribution: Dict[str, float] = field(default_factory=dict)
    membership_growth_rate: float = 0.0

    # Financials
    annual_revenue: float = 0.0
    sponsorship_revenue: float = 0.0
    financial_status: str = ""

    # Sponsorship history
    current_sponsors: List[str] = field(default_factory=list)
    sponsorship_packages: List[Dict[str, any]] = field(default_factory=list)
    sponsor_retention_rate: float = 0.0

    # Community engagement
    volunteer_count: int = 0
    fan_base_size: int = 0
    social_media_followers: Dict[str, int] = field(default_factory=dict)

    # Infrastructure
    owned_facilities: List[str] = field(default_factory=list)
    stadium_capacity: int = 0
    facility_conditions: Dict[str, str] = field(default_factory=dict)


================================================================================
FIL: sponsor_match/models/clustering.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
sponsor_match/models/clustering.py

Enhanced clustering implementation with multiple algorithms, validation metrics,
and proper integration with geocoded data. This replaces the basic K-means
with a more sophisticated approach.
"""

import json
import logging
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Optional, Union

import joblib
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler

from sponsor_match.core.config import DATA_DIR, MODELS_DIR, LOG_LEVEL

# Configure logging
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s %(levelname)s %(message)s"
)
logger = logging.getLogger(__name__)

# Constants
DEFAULT_N_CLUSTERS = 5
DBSCAN_MIN_SAMPLES = 5
MODEL_VERSION = "2.0"  # Track model versions for compatibility


@dataclass
class ClusteringMetrics:
    """Store clustering quality metrics."""
    silhouette_score: float
    calinski_harabasz_score: float
    davies_bouldin_score: float
    n_clusters: int
    n_noise_points: int = 0

    def to_dict(self) -> Dict:
        """Convert to dictionary for serialization."""
        return asdict(self)

    @property
    def quality_score(self) -> float:
        """
        Combined quality score (0-1 range).
        Higher silhouette and calinski_harabasz are better.
        Lower davies_bouldin is better.
        """
        # Normalize each metric to 0-1 range
        sil_norm = (self.silhouette_score + 1) / 2  # -1 to 1 -> 0 to 1

        # Calinski-Harabasz doesn't have a fixed range, use sigmoid
        ch_norm = 1 / (1 + np.exp(-self.calinski_harabasz_score / 1000))

        # Davies-Bouldin: lower is better, typical range 0-2
        db_norm = 1 - min(self.davies_bouldin_score / 2, 1)

        # Weighted average
        return 0.5 * sil_norm + 0.3 * ch_norm + 0.2 * db_norm


class GeographicClusteringModel:
    """
    Advanced clustering model for geographic sponsor-association matching.

    This class implements multiple clustering algorithms optimized for
    geographic data, with proper validation and metric tracking.
    """

    def __init__(
            self,
            algorithm: str = 'kmeans',
            n_clusters: int = DEFAULT_N_CLUSTERS,
            random_state: int = 42
    ):
        """
        Initialize clustering model.

        Args:
            algorithm: One of 'kmeans', 'dbscan', 'hierarchical'
            n_clusters: Number of clusters (ignored for DBSCAN)
            random_state: Random seed for reproducibility
        """
        self.algorithm = algorithm
        self.n_clusters = n_clusters
        self.random_state = random_state

        self.scaler = StandardScaler()
        self.model = None
        self.metrics_ = None
        self.feature_names_ = None
        self.model_metadata_ = {
            'version': MODEL_VERSION,
            'algorithm': algorithm,
            'trained': False
        }

    def fit(self, X: Union[np.ndarray, pd.DataFrame],
            feature_names: Optional[List[str]] = None) -> 'GeographicClusteringModel':
        """
        Fit the clustering model with automatic parameter optimization.

        Args:
            X: Feature matrix (n_samples, n_features)
            feature_names: Optional list of feature names

        Returns:
            Self for method chaining
        """
        # Convert to numpy array if needed
        if isinstance(X, pd.DataFrame):
            if feature_names is None:
                feature_names = X.columns.tolist()
            X = X.values

        self.feature_names_ = feature_names or [f'feature_{i}' for i in range(X.shape[1])]

        # Validate input
        if X.shape[0] < 2:
            raise ValueError("Need at least 2 samples for clustering")

        # Scale features
        X_scaled = self.scaler.fit_transform(X)

        # Train model based on algorithm
        if self.algorithm == 'kmeans':
            self._fit_kmeans(X_scaled)
        elif self.algorithm == 'dbscan':
            self._fit_dbscan(X_scaled)
        elif self.algorithm == 'hierarchical':
            self._fit_hierarchical(X_scaled)
        else:
            raise ValueError(f"Unknown algorithm: {self.algorithm}")

        # Calculate metrics
        self._calculate_metrics(X_scaled)

        # Update metadata
        self.model_metadata_['trained'] = True
        self.model_metadata_['n_samples'] = X.shape[0]
        self.model_metadata_['n_features'] = X.shape[1]

        logger.info(f"Trained {self.algorithm} model - Quality score: {self.metrics_.quality_score:.3f}")

        return self

    def _fit_kmeans(self, X_scaled: np.ndarray):
        """Fit K-means with optimal parameters."""
        # Determine optimal number of clusters if not specified
        if self.n_clusters == 'auto':
            self.n_clusters = self._find_optimal_clusters(X_scaled)

        self.model = KMeans(
            n_clusters=min(self.n_clusters, X_scaled.shape[0]),
            init='k-means++',
            n_init=10,
            max_iter=300,
            random_state=self.random_state
        )
        self.model.fit(X_scaled)

    def _fit_dbscan(self, X_scaled: np.ndarray):
        """Fit DBSCAN with adaptive epsilon."""
        # Find optimal epsilon using k-nearest neighbors
        eps = self._find_optimal_eps(X_scaled)

        self.model = DBSCAN(
            eps=eps,
            min_samples=min(DBSCAN_MIN_SAMPLES, X_scaled.shape[0] // 10),
            metric='euclidean',
            n_jobs=-1
        )
        self.model.fit(X_scaled)

        # DBSCAN doesn't have n_clusters attribute
        unique_labels = set(self.model.labels_)
        self.n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)

    def _fit_hierarchical(self, X_scaled: np.ndarray):
        """Fit Agglomerative Clustering."""
        self.model = AgglomerativeClustering(
            n_clusters=min(self.n_clusters, X_scaled.shape[0]),
            linkage='ward'
        )
        self.model.fit(X_scaled)

    def _find_optimal_clusters(self, X_scaled: np.ndarray, max_k: int = 10) -> int:
        """
        Find optimal number of clusters using elbow method and silhouette score.
        """
        max_k = min(max_k, X_scaled.shape[0] - 1)

        scores = []
        for k in range(2, max_k + 1):
            kmeans = KMeans(n_clusters=k, random_state=self.random_state)
            labels = kmeans.fit_predict(X_scaled)
            score = silhouette_score(X_scaled, labels)
            scores.append((k, score))

        # Find k with highest silhouette score
        optimal_k = max(scores, key=lambda x: x[1])[0]
        logger.info(f"Optimal clusters determined: {optimal_k}")

        return optimal_k

    def _find_optimal_eps(self, X_scaled: np.ndarray, n_neighbors: int = 5) -> float:
        """
        Find optimal DBSCAN epsilon using k-nearest neighbors.
        """
        nn = NearestNeighbors(n_neighbors=n_neighbors)
        nn.fit(X_scaled)
        distances, _ = nn.kneighbors(X_scaled)

        # Sort distances and find "elbow"
        sorted_distances = np.sort(distances[:, -1])

        # Use 90th percentile as epsilon
        eps = np.percentile(sorted_distances, 90)

        logger.info(f"Optimal epsilon determined: {eps:.4f}")
        return eps

    def _calculate_metrics(self, X_scaled: np.ndarray):
        """Calculate clustering quality metrics."""
        labels = self.predict_raw(X_scaled)

        # Count unique labels (excluding noise for DBSCAN)
        unique_labels = set(labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = np.sum(labels == -1)

        # Calculate metrics only if we have valid clusters
        if n_clusters < 2 or n_clusters >= len(labels):
            self.metrics_ = ClusteringMetrics(
                silhouette_score=0.0,
                calinski_harabasz_score=0.0,
                davies_bouldin_score=float('inf'),
                n_clusters=n_clusters,
                n_noise_points=n_noise
            )
        else:
            # Filter out noise points for metric calculation
            mask = labels != -1
            X_filtered = X_scaled[mask]
            labels_filtered = labels[mask]

            self.metrics_ = ClusteringMetrics(
                silhouette_score=silhouette_score(X_filtered, labels_filtered),
                calinski_harabasz_score=calinski_harabasz_score(X_filtered, labels_filtered),
                davies_bouldin_score=davies_bouldin_score(X_filtered, labels_filtered),
                n_clusters=n_clusters,
                n_noise_points=n_noise
            )

    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Predict cluster labels for new data.

        Args:
            X: Feature matrix

        Returns:
            Array of cluster labels
        """
        if not self.model_metadata_['trained']:
            raise ValueError("Model must be fitted before prediction")

        # Convert to numpy array if needed
        if isinstance(X, pd.DataFrame):
            X = X.values

        # Scale features
        X_scaled = self.scaler.transform(X)

        return self.predict_raw(X_scaled)

    def predict_raw(self, X_scaled: np.ndarray) -> np.ndarray:
        """Predict on already-scaled data."""
        if self.algorithm == 'kmeans':
            return self.model.predict(X_scaled)
        elif self.algorithm == 'dbscan':
            # DBSCAN doesn't have predict, use fit_predict for new data
            # For consistency, return cluster of nearest core point
            return self._predict_dbscan(X_scaled)
        elif self.algorithm == 'hierarchical':
            # Hierarchical doesn't have predict, find nearest cluster center
            return self._predict_hierarchical(X_scaled)

    def _predict_dbscan(self, X_scaled: np.ndarray) -> np.ndarray:
        """Predict DBSCAN clusters for new points."""
        # Find nearest core points
        core_mask = np.zeros(len(self.model.labels_), dtype=bool)
        core_mask[self.model.core_sample_indices_] = True

        # Get core points
        core_points = X_scaled[core_mask]
        core_labels = self.model.labels_[core_mask]

        # For each new point, find nearest core point
        predictions = []
        for point in X_scaled:
            distances = np.linalg.norm(core_points - point, axis=1)
            nearest_idx = np.argmin(distances)

            # Check if within eps distance
            if distances[nearest_idx] <= self.model.eps:
                predictions.append(core_labels[nearest_idx])
            else:
                predictions.append(-1)  # Noise

        return np.array(predictions)

    def _predict_hierarchical(self, X_scaled: np.ndarray) -> np.ndarray:
        """Predict hierarchical clusters for new points."""
        # Compute cluster centers
        centers = []
        for i in range(self.n_clusters):
            mask = self.model.labels_ == i
            if np.any(mask):
                centers.append(X_scaled[mask].mean(axis=0))

        centers = np.array(centers)

        # Assign to nearest center
        predictions = []
        for point in X_scaled:
            distances = np.linalg.norm(centers - point, axis=1)
            predictions.append(np.argmin(distances))

        return np.array(predictions)

    def save(self, filepath: Union[str, Path]):
        """
        Save the complete model to disk.

        Args:
            filepath: Path to save the model
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'algorithm': self.algorithm,
            'n_clusters': self.n_clusters,
            'metrics': self.metrics_.to_dict() if self.metrics_ else None,
            'feature_names': self.feature_names_,
            'metadata': self.model_metadata_
        }

        joblib.dump(model_data, filepath)
        logger.info(f"Saved model to {filepath}")

        # Also save metrics as JSON for easy inspection
        metrics_path = filepath.with_suffix('.metrics.json')
        with open(metrics_path, 'w') as f:
            json.dump({
                'algorithm': self.algorithm,
                'metrics': self.metrics_.to_dict() if self.metrics_ else None,
                'metadata': self.model_metadata_
            }, f, indent=2)

    @classmethod
    def load(cls, filepath: Union[str, Path]) -> 'GeographicClusteringModel':
        """
        Load a saved model from disk.

        Args:
            filepath: Path to the saved model

        Returns:
            Loaded model instance
        """
        filepath = Path(filepath)

        if not filepath.exists():
            raise FileNotFoundError(f"Model file not found: {filepath}")

        model_data = joblib.load(filepath)

        # Handle version compatibility
        metadata = model_data.get('metadata', {})
        version = metadata.get('version', '1.0')

        if version != MODEL_VERSION:
            logger.warning(f"Loading model version {version}, current version is {MODEL_VERSION}")

        # Reconstruct model
        instance = cls(
            algorithm=model_data['algorithm'],
            n_clusters=model_data.get('n_clusters', DEFAULT_N_CLUSTERS)
        )

        instance.model = model_data['model']
        instance.scaler = model_data['scaler']
        instance.feature_names_ = model_data.get('feature_names')
        instance.model_metadata_ = metadata

        # Reconstruct metrics if available
        metrics_dict = model_data.get('metrics')
        if metrics_dict:
            instance.metrics_ = ClusteringMetrics(**metrics_dict)

        return instance


def train_clustering_models_for_buckets():
    """
    Train separate clustering models for each size bucket using geocoded data.

    This function loads the geocoded CSVs and trains optimized models for
    different entity sizes (small, medium, large).
    """
    logger.info("Training clustering models with geocoded data...")

    # Load geocoded data
    associations_path = DATA_DIR / "associations_geocoded.csv"
    companies_path = DATA_DIR / "companies_geocoded.csv"

    if not associations_path.exists() or not companies_path.exists():
        logger.error("Geocoded CSV files not found. Please run geocoding first.")
        return

    # Load data
    associations_df = pd.read_csv(associations_path)
    companies_df = pd.read_csv(companies_path)

    # Rename columns for consistency
    if 'lat' in associations_df.columns:
        associations_df = associations_df.rename(columns={'lat': 'latitude', 'lon': 'longitude'})
    if 'lat' in companies_df.columns:
        companies_df = companies_df.rename(columns={'lat': 'latitude', 'lon': 'longitude'})

    # Combine all entities
    all_entities = pd.concat([
        associations_df[['latitude', 'longitude', 'size_bucket']],
        companies_df[['latitude', 'longitude', 'size_bucket']]
    ], ignore_index=True)

    # Filter valid coordinates
    valid_mask = (
            (all_entities['latitude'].between(-90, 90)) &
            (all_entities['longitude'].between(-180, 180))
    )
    all_entities = all_entities[valid_mask]

    logger.info(f"Total valid entities for clustering: {len(all_entities)}")

    # Train models for each size bucket
    models = {}

    for bucket in ['small', 'medium', 'large']:
        bucket_data = all_entities[all_entities['size_bucket'] == bucket]

        if len(bucket_data) < 5:
            logger.warning(f"Insufficient data for {bucket} bucket ({len(bucket_data)} entities)")
            continue

        logger.info(f"\nTraining {bucket} model with {len(bucket_data)} entities...")

        # Prepare features
        features = bucket_data[['latitude', 'longitude']].values

        # Try different algorithms and pick the best
        best_model = None
        best_score = -1

        for algorithm in ['kmeans', 'dbscan', 'hierarchical']:
            try:
                model = GeographicClusteringModel(
                    algorithm=algorithm,
                    n_clusters=min(10, len(bucket_data) // 5)
                )
                model.fit(features, feature_names=['latitude', 'longitude'])

                if model.metrics_.quality_score > best_score:
                    best_score = model.metrics_.quality_score
                    best_model = model

                logger.info(f"  {algorithm}: quality score = {model.metrics_.quality_score:.3f}")

            except Exception as e:
                logger.warning(f"  {algorithm} failed: {e}")

        if best_model:
            # Save the best model
            model_path = MODELS_DIR / f"clustering_{bucket}.joblib"
            best_model.save(model_path)
            models[bucket] = best_model

            logger.info(f"  Best model: {best_model.algorithm} (score: {best_score:.3f})")

    # Also train a combined "default" model for backward compatibility
    logger.info("\nTraining default combined model...")

    default_data = all_entities[all_entities['size_bucket'].isin(['small', 'medium'])]
    if len(default_data) >= 5:
        features = default_data[['latitude', 'longitude']].values

        default_model = GeographicClusteringModel(
            algorithm='kmeans',
            n_clusters=min(10, len(default_data) // 10)
        )
        default_model.fit(features, feature_names=['latitude', 'longitude'])
        default_model.save(MODELS_DIR / "kmeans.joblib")

        logger.info(f"Default model trained with {len(default_data)} entities")

    return models


def analyze_clustering_quality():
    """
    Analyze and report on the quality of existing clustering models.

    This function loads saved models and provides detailed quality metrics.
    """
    logger.info("Analyzing clustering model quality...")

    models_to_check = [
        ('Default', MODELS_DIR / "kmeans.joblib"),
        ('Small', MODELS_DIR / "clustering_small.joblib"),
        ('Medium', MODELS_DIR / "clustering_medium.joblib"),
        ('Large', MODELS_DIR / "clustering_large.joblib"),
    ]

    results = []

    for name, path in models_to_check:
        if not path.exists():
            logger.warning(f"{name} model not found at {path}")
            continue

        try:
            model = GeographicClusteringModel.load(path)

            if model.metrics_:
                results.append({
                    'name': name,
                    'algorithm': model.algorithm,
                    'n_clusters': model.n_clusters,
                    'quality_score': model.metrics_.quality_score,
                    'silhouette': model.metrics_.silhouette_score,
                    'n_noise': model.metrics_.n_noise_points
                })

                logger.info(f"\n{name} Model:")
                logger.info(f"  Algorithm: {model.algorithm}")
                logger.info(f"  Clusters: {model.n_clusters}")
                logger.info(f"  Quality Score: {model.metrics_.quality_score:.3f}")
                logger.info(f"  Silhouette: {model.metrics_.silhouette_score:.3f}")

                if model.metrics_.n_noise_points > 0:
                    logger.info(f"  Noise Points: {model.metrics_.n_noise_points}")

        except Exception as e:
            logger.error(f"Failed to analyze {name} model: {e}")

    # Summary
    if results:
        avg_quality = np.mean([r['quality_score'] for r in results])
        logger.info(f"\nOverall average quality score: {avg_quality:.3f}")

        if avg_quality < 0.5:
            logger.warning("Low clustering quality detected. Consider retraining with different parameters.")

    return results


# Main execution
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Clustering model management")
    parser.add_argument('--train', action='store_true', help='Train new models')
    parser.add_argument('--analyze', action='store_true', help='Analyze existing models')
    parser.add_argument('--algorithm', choices=['kmeans', 'dbscan', 'hierarchical'],
                        default='kmeans', help='Algorithm to use for training')

    args = parser.parse_args()

    if args.train:
        train_clustering_models_for_buckets()
    elif args.analyze:
        analyze_clustering_quality()
    else:
        parser.print_help()


================================================================================
FIL: sponsor_match/models/entities.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
models/entities.py
------------------
Domain entity classes for SponsorMatch AI with proper SQLAlchemy ORM mappings.
"""

from sqlalchemy import Column, Integer, String, Float, Enum
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()


class Association(Base):
    """
    Represents a sports club/association as stored in the `associations` table.
    """
    __tablename__ = 'associations'

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(120))
    member_count = Column(Integer)
    address = Column(String(255))
    lat = Column(Float)
    lon = Column(Float)
    size_bucket = Column(Enum('small', 'medium', 'large'))
    founded_year = Column(Integer)


class Company(Base):
    """
    Represents a company as stored in the `companies` table.
    """
    __tablename__ = 'companies'

    id = Column(Integer, primary_key=True, autoincrement=True)
    orgnr = Column(String(10))
    name = Column(String(200))
    revenue_ksek = Column(Float)
    employees = Column(Integer)
    year = Column(Integer)
    size_bucket = Column(Enum('small', 'medium', 'large'))
    industry = Column(String(120))
    lat = Column(Float)
    lon = Column(Float)


# Legacy dataclasses for backward compatibility
from dataclasses import dataclass
from typing import Optional


@dataclass
class Club:
    """
    Legacy dataclass representation of a sports club.
    Use Association ORM model for database operations.
    """
    id: int
    name: str
    member_count: int
    address: str
    lat: Optional[float]
    lon: Optional[float]
    size_bucket: str
    founded_year: int


@dataclass
class CompanyData:
    """
    Legacy dataclass representation of a company.
    Use Company ORM model for database operations.
    """
    id: int
    orgnr: str
    name: str
    revenue_ksek: float
    employees: int
    year: int
    size_bucket: str
    lat: Optional[float]
    lon: Optional[float]
    industry: str

================================================================================
FIL: sponsor_match/models/features.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
sponsor_match/models/features.py
--------------------------------
Feature engineering for SponsorMatch ML models.

This module provides comprehensive feature engineering capabilities for analyzing
and scoring potential sponsor-club matches based on multiple dimensions including
geographic proximity, size compatibility, and industry relevance.
"""

from datetime import datetime
from typing import Dict

import numpy as np
import pandas as pd
from geopy.distance import geodesic


class FeatureEngineer:
    """
    Compute pairwise features between clubs and companies for recommendation ranking.

    This class contains methods for calculating various features used in matching
    sponsors with clubs, including geographic distance, size compatibility, industry
    affinity, and economic indicators.
    """

    @staticmethod
    def calculate_distance_km(
        lat1: float, lon1: float, lat2: float, lon2: float
    ) -> float:
        """
        Calculate the geodesic distance in kilometers between two latitude/longitude points.

        Parameters
        ----------
        lat1, lon1 : float
            Latitude and longitude of the first point.
        lat2, lon2 : float
            Latitude and longitude of the second point.

        Returns
        -------
        float
            Distance in kilometers.

        Raises
        ------
        TypeError
            If any coordinates are None.
        ValueError
            If any coordinates are zero or negative, or otherwise invalid.
        """
        # 1) None check
        if any(x is None for x in (lat1, lon1, lat2, lon2)):
            raise TypeError("Coordinates cannot be None")

        # 2) Domain check: all lats/lons must be positive in our context
        if any(v <= 0 for v in (lat1, lon1, lat2, lon2)):
            raise ValueError(f"Invalid coordinates for distance calculation: {(lat1, lon1, lat2, lon2)}")

        try:
            return geodesic((lat1, lon1), (lat2, lon2)).km
        except (ValueError, TypeError) as e:
            # Re-raise with more descriptive message
            raise type(e)(f"Invalid coordinates for distance calculation: {e}")

    @staticmethod
    def add_distance(
        df: pd.DataFrame,
        lat: float,
        lon: float,
        lat_col: str = "lat",
        lon_col: str = "lon",
        new_col: str = "distance_km",
    ) -> pd.DataFrame:
        """
        Return a copy of `df` with a new column `new_col` representing the distance
        from the fixed point (`lat`, `lon`) to each row's (lat_col, lon_col).
        """
        df_copy = df.copy()
        df_copy[new_col] = df_copy.apply(
            lambda row: FeatureEngineer.calculate_distance_km(
                lat, lon, row[lat_col], row[lon_col]
            ),
            axis=1,
        )
        return df_copy

    @staticmethod
    def bucket_assoc_size(members: int) -> str:
        """
        Bucket a club's member count into 'small', 'medium', or 'large'.
        """
        if members < 200:
            return "small"
        if members < 1000:
            return "medium"
        return "large"

    @staticmethod
    def calculate_distance(
        club_coords: pd.DataFrame,
        comp_coords: pd.DataFrame
    ) -> pd.Series:
        """
        Compute geodesic distance (km) between each club–company pair (row-wise).
        """
        if len(club_coords) != len(comp_coords):
            raise ValueError("Input DataFrames must have the same length")

        distances = [
            FeatureEngineer.calculate_distance_km(
                club_coords.iloc[i]["lat"],
                club_coords.iloc[i]["lon"],
                comp_coords.iloc[i]["lat"],
                comp_coords.iloc[i]["lon"],
            )
            for i in range(len(club_coords))
        ]
        return pd.Series(distances, name="distance_km")

    @staticmethod
    def calculate_size_match(
        club_sizes: pd.Series,
        comp_sizes: pd.Series
    ) -> pd.Series:
        """
        Score size compatibility: exact match → 1.0; adjacent → 0.5; else → 0.0.
        """
        size_map = {"small": 0, "medium": 1, "large": 2}

        def _score(cs, ps):
            a = size_map.get(cs, 0)
            b = size_map.get(ps, 0)
            if a == b:
                return 1.0
            if abs(a - b) == 1:
                return 0.5
            return 0.0

        scores = [_score(c, p) for c, p in zip(club_sizes, comp_sizes)]
        return pd.Series(scores, name="size_match")

    @staticmethod
    def calculate_industry_affinity(
        sport_types: pd.Series,
        industries: pd.Series
    ) -> pd.Series:
        """
        Calculate industry-sport affinity score (0.0 or 1.0).
        """
        def _affinity(sports, industry):
            if not isinstance(sports, list) or not isinstance(industry, str):
                return 0.0
            return 1.0 if any(sp.lower() in industry.lower() for sp in sports) else 0.0

        affinities = [_affinity(s, i) for s, i in zip(sport_types, industries)]
        return pd.Series(affinities, name="industry_sport_affinity")

    @staticmethod
    def calculate_growth_rate(
        companies_df: pd.DataFrame
    ) -> pd.Series:
        """
        Placeholder for company growth rate; returns zeros until time-series data is available.
        """
        return pd.Series(0.0, index=companies_df.index, name="growth_rate")

    @staticmethod
    def urban_rural_compatibility(
        club_loc: pd.Series,
        comp_loc: pd.Series
    ) -> pd.Series:
        """
        Binary match if club and company share the same location_type.
        """
        compat = [1.0 if cl == cp else 0.0 for cl, cp in zip(club_loc, comp_loc)]
        return pd.Series(compat, name="urban_rural_match")

    @classmethod
    def make_pair_features(cls, df: pd.DataFrame) -> pd.DataFrame:
        """
        Create a feature DataFrame for club–company pairs for the matching model.

        Returns features:
          - distance_km: exact geodesic distance
          - size_match: size compatibility score
          - revenue_ksek: raw revenue value (in tkr)
          - employees: raw employee count
          - distance_score: exp(-distance_km/50) decay
        """
        features: Dict[str, pd.Series] = {}

        # Distance
        if all(col in df.columns for col in ["club_lat", "club_lon", "company_lat", "company_lon"]):
            features["distance_km"] = df.apply(
                lambda r: cls.calculate_distance_km(
                    r["club_lat"], r["club_lon"],
                    r["company_lat"], r["company_lon"]
                ),
                axis=1
            )

        # Size compatibility
        if "club_size" in df.columns and "company_size" in df.columns:
            features["size_match"] = cls.calculate_size_match(
                df["club_size"], df["company_size"]
            )

        # Raw financial and headcount features
        if "revenue_ksek" in df.columns:
            features["revenue_ksek"] = df["revenue_ksek"].rename("revenue_ksek")
        if "employees" in df.columns:
            features["employees"] = df["employees"].rename("employees")

        # Exponential distance decay
        if "distance_km" in features:
            features["distance_score"] = np.exp(-features["distance_km"] / 50)

        return pd.DataFrame(features)

    def create_features(
        self,
        clubs_df: pd.DataFrame,
        companies_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Build a comprehensive feature set for each club–company pair.

        Returns features including:
          - distance_km, distance_score
          - size_match
          - industry_sport_affinity
          - revenue_ksek, employees
          - company_age
          - growth_rate
          - urban_rural_match
        """
        feats: Dict[str, pd.Series] = {}

        # 1) Distance & decay
        if {"lat", "lon"}.issubset(clubs_df.columns) and {"lat", "lon"}.issubset(companies_df.columns):
            club_coords = clubs_df[["lat", "lon"]]
            comp_coords = companies_df[["lat", "lon"]]
            feats["distance_km"] = self.calculate_distance(club_coords, comp_coords)
            feats["distance_score"] = np.exp(-feats["distance_km"] / 50)

        # 2) Size match
        if "size_bucket" in clubs_df.columns and "size_bucket" in companies_df.columns:
            feats["size_match"] = self.calculate_size_match(
                clubs_df["size_bucket"], companies_df["size_bucket"]
            )

        # 3) Industry affinity
        if "sport_types" in clubs_df.columns and "industry" in companies_df.columns:
            feats["industry_sport_affinity"] = self.calculate_industry_affinity(
                clubs_df["sport_types"], companies_df["industry"]
            )

        # 4) Raw financial & headcount
        if "revenue_ksek" in companies_df.columns:
            feats["revenue_ksek"] = companies_df["revenue_ksek"].rename("revenue_ksek")
        if "employees" in companies_df.columns:
            feats["employees"] = companies_df["employees"].rename("employees")

        # 5) Company age
        if "founded_year" in companies_df.columns:
            feats["company_age"] = (
                datetime.now().year - companies_df["founded_year"]
            ).rename("company_age")

        # 6) Growth rate
        if "employees" in companies_df.columns:
            feats["growth_rate"] = self.calculate_growth_rate(companies_df)

        # 7) Urban/rural match
        if "location_type" in clubs_df.columns and "location_type" in companies_df.columns:
            feats["urban_rural_match"] = self.urban_rural_compatibility(
                clubs_df["location_type"], companies_df["location_type"]
            )

        return pd.DataFrame(feats)


================================================================================
FIL: sponsor_match/models/models.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
sponsor_match/models/models.py
-------------------------------
Ensemble of ML models for sponsorship probability prediction.
"""

import logging
from typing import Any, Dict, Union

import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from sklearn.neural_network import MLPRegressor

logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)

class SponsorshipPredictorEnsemble:
    """
    Holds a collection of models and provides a unified interface
    for training and predicting sponsor-match probabilities.
    """

    def __init__(self) -> None:
        """
        Initialize the ensemble with default hyperparameters.
        """
        self.models: Dict[str, Any] = {
            "rf": RandomForestRegressor(n_estimators=100),
            "gbm": GradientBoostingClassifier(),
            "lgbm": lgb.LGBMRegressor(),
            "nn": MLPRegressor(hidden_layer_sizes=(100, 50))
        }
        logger.info("Initialized SponsorshipPredictorEnsemble with models: %s",
                    list(self.models.keys()))

    def train(
        self,
        X_train: Union[pd.DataFrame, np.ndarray],
        y_train: Union[pd.Series, np.ndarray]
    ) -> None:
        """
        Fit each model in the ensemble on the training data.

        Parameters
        ----------
        X_train : DataFrame or ndarray
            Feature matrix.
        y_train : Series or ndarray
            Binary labels (1 = sponsored before, 0 = not).
        """
        for name, model in self.models.items():
            logger.info("Training model '%s'", name)
            model.fit(X_train, y_train)
        logger.info("All models trained successfully")

    def predict_proba(
        self,
        X: Union[pd.DataFrame, np.ndarray]
    ) -> np.ndarray:
        """
        Return the average predicted probability of sponsorship
        across all models in the ensemble.

        Parameters
        ----------
        X : DataFrame or ndarray
            Feature matrix.

        Returns
        -------
        ndarray
            Array of probabilities, one per row in X.
        """
        prob_list = []
        for name, model in self.models.items():
            if hasattr(model, "predict_proba"):
                probs = model.predict_proba(X)[:, 1]
                logger.debug("Model '%s' provided predict_proba output", name)
            else:
                # fallback: normalize regression output into [0,1]
                raw = model.predict(X)
                min_, max_ = raw.min(), raw.max()
                if max_ - min_ > 1e-8:
                    probs = (raw - min_) / (max_ - min_)
                else:
                    probs = np.zeros_like(raw)
                logger.debug("Model '%s' provided normalized regression output", name)
            prob_list.append(probs)

        # Ensemble by averaging
        ensemble_probs = np.mean(prob_list, axis=0)
        logger.info("Ensembled probabilities computed (shape=%s)", ensemble_probs.shape)
        return ensemble_probs


================================================================================
FIL: sponsor_match/data/__init__.py
================================================================================



================================================================================
FIL: sponsor_match/data/geocode_with_municipality.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
sponsor_match/data/geocode_with_municipality.py

Comprehensive geocoding system using Göteborg municipality data.
Handles associations and companies with various address formats.
"""

import logging
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple, Dict, List

import numpy as np
import pandas as pd
from fuzzywuzzy import fuzz, process
from geopy.extra.rate_limiter import RateLimiter
from geopy.geocoders import Nominatim

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class GeocodingResult:
    """Store geocoding results with confidence metadata."""
    lat: float
    lon: float
    district: str
    confidence: str  # 'exact', 'fuzzy', 'postcode', 'box', 'external'
    match_details: str
    original_address: str


class MunicipalityGeocoder:
    """
    Intelligent geocoder using Göteborg municipality data as primary source.
    Falls back to external services only when necessary.
    """

    def __init__(self, municipality_csv_path: str):
        """Initialize with municipality data."""
        logger.info("Loading municipality data...")
        # Try reading with UTF-8 encoding first, fall back to Latin-1 if needed
        try:
            self.muni_df = pd.read_csv(municipality_csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            logger.warning("UTF-8 decoding failed, trying Latin-1...")
            self.muni_df = pd.read_csv(municipality_csv_path, encoding='latin-1')

        # Fix encoding issues if they exist
        self._fix_encoding_issues()

        # Create various indices for efficient lookups
        self._prepare_indices()

        # Initialize external geocoder as fallback
        geolocator = Nominatim(user_agent="sponsor_match_geocoder")
        self.external_geocoder = RateLimiter(geolocator.geocode, min_delay_seconds=1)

        # Cache for Box addresses (will be populated on first use)
        self.box_address_cache = {}

        # Settings for controlling external geocoding
        self.use_external_geocoding = False  # Disable by default to avoid timeouts

    def _fix_encoding_issues(self):
        """Fix common UTF-8/Latin-1 encoding issues in Swedish text."""
        # Common misencoded Swedish character patterns
        encoding_fixes = {
            'ã¥': 'å', 'Ã¥': 'Å',
            'ã¤': 'ä', 'Ã¤': 'Ä',
            'ã¶': 'ö', 'Ã¶': 'Ö',
            'ã©': 'é', 'Ã©': 'É',
            'ã': 'à', 'Ã': 'À'
        }

        # Apply fixes to string columns
        for col in self.muni_df.columns:
            if self.muni_df[col].dtype == 'object':  # String columns
                for wrong, right in encoding_fixes.items():
                    self.muni_df[col] = self.muni_df[col].str.replace(wrong, right, regex=False)

        logger.info("Applied encoding fixes to municipality data")

    def _prepare_indices(self):
        """Prepare various indices for fast lookups."""
        # Clean and standardize street names
        self.muni_df['street_clean'] = (
            self.muni_df['STREET']
            .str.lower()
            .str.strip()
            .fillna('')
        )

        # Create full address for matching
        self.muni_df['full_address'] = (
                self.muni_df['street_clean'] + ' ' +
                self.muni_df['NUMBER'].fillna('').astype(str)
        ).str.strip()

        # Group by street for street-level lookups
        self.street_groups = self.muni_df.groupby('street_clean')

        # Create postcode index
        self.postcode_groups = self.muni_df.groupby('POSTCODE')

        # Get unique streets for fuzzy matching
        self.unique_streets = self.muni_df['street_clean'].unique()

        logger.info(f"Indexed {len(self.muni_df)} addresses across {len(self.unique_streets)} streets")

    def parse_address(self, raw_address: str) -> Dict[str, str]:
        """
        Parse various address formats into components.
        Handles contact names, Box addresses, and standard formats.
        """
        # Handle None or empty addresses
        if not raw_address or pd.isna(raw_address):
            return {'street': '', 'number': '', 'postcode': '', 'city': ''}

        address = str(raw_address).strip()

        # Check if it's a Box address
        box_match = re.search(r'Box\s*(\d+)', address, re.IGNORECASE)
        if box_match:
            return {
                'street': 'BOX',
                'number': box_match.group(1),
                'postcode': self._extract_postcode(address),
                'city': self._extract_city(address),
                'is_box': True
            }

        # Split by comma to separate components
        parts = [p.strip() for p in address.split(',')]

        # For associations, the middle part is often a contact name - skip it
        # Pattern: Street, ContactName, PostCode, City
        if len(parts) >= 4:
            street_part = parts[0]
            # Skip parts[1] as it's likely a contact name
            # Reconstruct address without contact name
            address = f"{parts[0]}, {parts[2]}, {parts[3]}"
        elif len(parts) >= 1:
            street_part = parts[0]
        else:
            street_part = ''

        # Extract postcode (5 digits, possibly with space)
        postcode = self._extract_postcode(address)

        # Extract house number from street part
        street, number = self._split_street_number(street_part)

        # Last part after postcode is usually city
        city = self._extract_city(address)

        return {
            'street': street,
            'number': number,
            'postcode': postcode,
            'city': city,
            'is_box': False
        }

    def _extract_postcode(self, address: str) -> str:
        """Extract Swedish postcode from address."""
        # Match 5 digits with optional space (e.g., "412 76" or "41276")
        match = re.search(r'(\d{3}\s?\d{2})', address)
        if match:
            # Normalize to no-space format
            return match.group(1).replace(' ', '')
        return ''

    def _extract_city(self, address: str) -> str:
        """Extract city name from address."""
        # Common districts/cities in Göteborg area
        cities = ['Göteborg', 'GÖTEBORG', 'Mölndal', 'MÖLNDAL', 'Partille', 'PARTILLE',
                  'Angered', 'ANGERED', 'Västra Frölunda', 'VÄSTRA FRÖLUNDA',
                  'Hisings Backa', 'HISINGS BACKA', 'Torslanda', 'TORSLANDA']

        for city in cities:
            if city in address:
                return city.title()
        return 'Göteborg'  # Default

    def _split_street_number(self, street_part: str) -> Tuple[str, str]:
        """Split street name and house number."""
        # Try to find number at the end
        match = re.match(r'^(.+?)\s+(\d+\s*[A-Za-z]?)$', street_part)
        if match:
            return match.group(1).strip(), match.group(2).strip()

        # No number found
        return street_part.strip(), ''

    def geocode_address(self, raw_address: str) -> Optional[GeocodingResult]:
        """
        Main geocoding method with multiple fallback strategies.
        """
        # Parse the address
        parsed = self.parse_address(raw_address)

        # Handle Box addresses specially
        if parsed.get('is_box'):
            return self._geocode_box_address(parsed, raw_address)

        # Try exact match first
        result = self._try_exact_match(parsed, raw_address)
        if result:
            return result

        # Try fuzzy street matching
        result = self._try_fuzzy_match(parsed, raw_address)
        if result:
            return result

        # Try postcode-based geocoding
        result = self._try_postcode_match(parsed, raw_address)
        if result:
            return result

        # Last resort: external geocoding
        return self._try_external_geocoding(raw_address)

    def _try_exact_match(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """Try exact street + number match."""
        street_clean = parsed['street'].lower().strip()
        number = parsed['number']

        if not street_clean:
            return None

        # Get all addresses on this street
        if street_clean in self.street_groups.groups:
            street_data = self.street_groups.get_group(street_clean)

            if number:
                # Try exact number match
                exact_match = street_data[street_data['NUMBER'].astype(str) == str(number)]
                if not exact_match.empty:
                    row = exact_match.iloc[0]
                    return GeocodingResult(
                        lat=row['LAT'],
                        lon=row['LON'],
                        district=row['DISTRICT'],
                        confidence='exact',
                        match_details=f"Exact match: {row['STREET']} {row['NUMBER']}",
                        original_address=original
                    )

            # No number or no exact match - use street midpoint
            return self._get_street_midpoint(street_data, parsed, original)

        return None

    def _try_fuzzy_match(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """Try fuzzy matching for street names."""
        street_clean = parsed['street'].lower().strip()
        if not street_clean or len(street_clean) < 3:
            return None

        # Find best matching street
        best_match = process.extractOne(
            street_clean,
            self.unique_streets,
            scorer=fuzz.ratio,
            score_cutoff=70  # Lower threshold to handle encoding issues
        )

        if best_match:
            matched_street, score = best_match[0], best_match[1]
            street_data = self.street_groups.get_group(matched_street)

            logger.info(f"Fuzzy matched '{street_clean}' to '{matched_street}' (score: {score})")

            # Try to find specific number or use midpoint
            if parsed['number']:
                number_match = street_data[street_data['NUMBER'].astype(str) == str(parsed['number'])]
                if not number_match.empty:
                    row = number_match.iloc[0]
                    return GeocodingResult(
                        lat=row['LAT'],
                        lon=row['LON'],
                        district=row['DISTRICT'],
                        confidence='fuzzy',
                        match_details=f"Fuzzy match: {row['STREET']} {row['NUMBER']} (score: {score})",
                        original_address=original
                    )

            return self._get_street_midpoint(street_data, parsed, original, confidence='fuzzy')

        return None

    def _get_street_midpoint(self, street_data: pd.DataFrame, parsed: Dict,
                             original: str, confidence: str = 'exact') -> GeocodingResult:
        """Calculate midpoint of all addresses on a street."""
        # Use the centroid of all points on the street
        lat_mean = street_data['LAT'].mean()
        lon_mean = street_data['LON'].mean()

        # Use most common district
        district = street_data['DISTRICT'].mode().iloc[0] if not street_data['DISTRICT'].mode().empty else ''

        return GeocodingResult(
            lat=lat_mean,
            lon=lon_mean,
            district=district,
            confidence=confidence,
            match_details=f"Street midpoint: {street_data.iloc[0]['STREET']} ({len(street_data)} addresses)",
            original_address=original
        )

    def _try_postcode_match(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """Use postcode centroid as fallback."""
        postcode = parsed['postcode']
        if not postcode:
            return None

        if postcode in self.postcode_groups.groups:
            postcode_data = self.postcode_groups.get_group(postcode)

            # Use centroid of all addresses in this postcode
            lat_mean = postcode_data['LAT'].mean()
            lon_mean = postcode_data['LON'].mean()
            district = postcode_data['DISTRICT'].mode().iloc[0] if not postcode_data['DISTRICT'].mode().empty else ''

            return GeocodingResult(
                lat=lat_mean,
                lon=lon_mean,
                district=district,
                confidence='postcode',
                match_details=f"Postcode centroid: {postcode} ({len(postcode_data)} addresses)",
                original_address=original
            )

        return None

    def _geocode_box_address(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """
        Handle Box addresses. In Sweden, these are typically at post offices.
        We'll need external geocoding for these.
        """
        box_number = parsed['number']
        postcode = parsed['postcode']
        city = parsed['city']

        # Create a searchable address
        search_address = f"Box {box_number}, {postcode} {city}, Sverige"

        # Check cache first
        if search_address in self.box_address_cache:
            cached = self.box_address_cache[search_address]
            return GeocodingResult(
                lat=cached['lat'],
                lon=cached['lon'],
                district=cached.get('district', ''),
                confidence='box',
                match_details=f"Box address (cached): {search_address}",
                original_address=original
            )

        # Try external geocoding
        result = self._try_external_geocoding(search_address)
        if result:
            result.confidence = 'box'
            result.match_details = f"Box address (geocoded): {search_address}"
            # Cache for future use
            self.box_address_cache[search_address] = {
                'lat': result.lat,
                'lon': result.lon,
                'district': result.district
            }

        return result

    def _try_external_geocoding(self, address: str) -> Optional[GeocodingResult]:
        """Use external geocoding service as last resort."""
        # Check if external geocoding is enabled
        if not self.use_external_geocoding:
            logger.debug(f"External geocoding disabled, skipping: {address}")
            return None

        try:
            # Add country for better results
            if "Sverige" not in address and "Sweden" not in address:
                address += ", Sverige"

            location = self.external_geocoder(address)
            if location:
                # Try to determine district from coordinates
                district = self._find_district_for_coords(location.latitude, location.longitude)

                return GeocodingResult(
                    lat=location.latitude,
                    lon=location.longitude,
                    district=district,
                    confidence='external',
                    match_details=f"External geocoding: {location.address}",
                    original_address=address
                )
        except Exception as e:
            logger.error(f"External geocoding failed for '{address}': {e}")

        return None

    def _find_district_for_coords(self, lat: float, lon: float, radius_km: float = 1.0) -> str:
        """Find district for given coordinates by finding nearest address."""
        # Calculate distances to all addresses (simplified, not haversine)
        self.muni_df['dist'] = np.sqrt(
            (self.muni_df['LAT'] - lat) ** 2 +
            (self.muni_df['LON'] - lon) ** 2
        )

        # Find nearest address
        nearest = self.muni_df.nsmallest(1, 'dist')
        if not nearest.empty:
            return nearest.iloc[0]['DISTRICT']

        return ''

    def geocode_batch(self, addresses: List[str], desc: str = "Geocoding") -> List[Optional[GeocodingResult]]:
        """Geocode a batch of addresses with progress tracking."""
        results = []

        logger.info(f"Starting batch geocoding of {len(addresses)} addresses...")

        for i, address in enumerate(addresses):
            if i % 100 == 0:
                logger.info(f"{desc}: {i}/{len(addresses)} ({i / len(addresses) * 100:.1f}%)")

            result = self.geocode_address(address)
            results.append(result)

            # Small delay to prevent overwhelming external service
            if result and result.confidence == 'external':
                time.sleep(0.1)

        # Summary statistics
        confidences = [r.confidence for r in results if r]
        for conf in ['exact', 'fuzzy', 'postcode', 'box', 'external']:
            count = confidences.count(conf)
            if count > 0:
                logger.info(f"  {conf}: {count} ({count / len(addresses) * 100:.1f}%)")

        failed = len([r for r in results if r is None])
        if failed > 0:
            logger.warning(f"  Failed: {failed} ({failed / len(addresses) * 100:.1f}%)")

        return results


def geocode_associations(associations_csv: str, municipality_csv: str, output_csv: str):
    """Geocode all associations and save results."""
    logger.info("Loading associations...")
    assoc_df = pd.read_csv(associations_csv, encoding='utf-8')

    # Initialize geocoder
    geocoder = MunicipalityGeocoder(municipality_csv)

    # Extract addresses (handling the specific format)
    addresses = []
    for _, row in assoc_df.iterrows():
        # Combine address components, skipping Co Adress
        address_parts = []
        if pd.notna(row.get('Adress')):
            address_parts.append(str(row['Adress']))
        if pd.notna(row.get('Post Nr')):
            address_parts.append(str(row['Post Nr']))
        if pd.notna(row.get('Postort')):
            address_parts.append(str(row['Postort']))

        address = ', '.join(address_parts)
        addresses.append(address)

    # Geocode all addresses
    results = geocoder.geocode_batch(addresses, desc="Associations")

    # Add results to dataframe
    assoc_df['lat'] = [r.lat if r else None for r in results]
    assoc_df['lon'] = [r.lon if r else None for r in results]
    assoc_df['district'] = [r.district if r else None for r in results]
    assoc_df['geocoding_confidence'] = [r.confidence if r else 'failed' for r in results]
    assoc_df['geocoding_details'] = [r.match_details if r else 'Geocoding failed' for r in results]

    # Save results
    assoc_df.to_csv(output_csv, index=False, encoding='utf-8')
    logger.info(f"Saved geocoded associations to {output_csv}")

    # Report success rate
    success_rate = (assoc_df['lat'].notna().sum() / len(assoc_df)) * 100
    logger.info(f"Successfully geocoded {success_rate:.1f}% of associations")


def geocode_companies(companies_csv: str, municipality_csv: str, output_csv: str):
    """Geocode all companies and save results."""
    logger.info("Loading companies...")
    comp_df = pd.read_csv(companies_csv, encoding='utf-8')

    # Initialize geocoder
    geocoder = MunicipalityGeocoder(municipality_csv)

    # Extract addresses
    addresses = comp_df['registered_address'].tolist()

    # Geocode all addresses
    results = geocoder.geocode_batch(addresses, desc="Companies")

    # Add results to dataframe
    comp_df['lat'] = [r.lat if r else None for r in results]
    comp_df['lon'] = [r.lon if r else None for r in results]
    comp_df['geocoding_confidence'] = [r.confidence if r else 'failed' for r in results]
    comp_df['geocoding_details'] = [r.match_details if r else 'Geocoding failed' for r in results]

    # Verify district matches
    comp_df['district_verified'] = [
        r.district == row['district'] if r and r.district else False
        for r, (_, row) in zip(results, comp_df.iterrows())
    ]

    # Save results
    comp_df.to_csv(output_csv, index=False, encoding='utf-8')
    logger.info(f"Saved geocoded companies to {output_csv}")

    # Report success rate
    success_rate = (comp_df['lat'].notna().sum() / len(comp_df)) * 100
    logger.info(f"Successfully geocoded {success_rate:.1f}% of companies")


def main():
    """Main function to geocode both associations and companies."""
    # Set up paths
    project_root = Path(__file__).parent.parent.parent
    data_dir = project_root / "data"

    municipality_csv = data_dir / "municipality_of_goteborg.csv"
    associations_csv = data_dir / "gothenburg_associations.csv"
    companies_csv = data_dir / "gothenburg_companies_addresses.csv"

    # Output files
    associations_output = data_dir / "associations_geocoded.csv"
    companies_output = data_dir / "companies_geocoded.csv"

    # Geocode associations
    logger.info("=" * 50)
    logger.info("GEOCODING ASSOCIATIONS")
    logger.info("=" * 50)
    geocode_associations(str(associations_csv), str(municipality_csv), str(associations_output))

    # Geocode companies
    logger.info("\n" + "=" * 50)
    logger.info("GEOCODING COMPANIES")
    logger.info("=" * 50)
    geocode_companies(str(companies_csv), str(municipality_csv), str(companies_output))

    logger.info("\nGeocoding complete!")


if __name__ == "__main__":
    main()


================================================================================
FIL: sponsor_match/data/ingest_associations_fixed.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
Ingest Gothenburg associations with geocoding and proper data cleaning.
"""

import logging
import time
from pathlib import Path

import pandas as pd
from geopy.extra.rate_limiter import RateLimiter
from geopy.geocoders import Nominatim
from sqlalchemy import text

from sponsor_match.core.db import get_engine
from sponsor_match.models.entities import Base

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def clean_associations_data(df):
    """Clean and prepare associations data."""
    # Create full address
    df['full_address'] = df['Adress'].fillna('') + ', ' + \
                         df['Post Nr'].fillna('').astype(str) + ' ' + \
                         df['Postort'].fillna('')

    # Clean up whitespace
    df['full_address'] = df['full_address'].str.strip().str.replace('  ', ' ')

    # Add ", Sverige" for better geocoding
    df['full_address'] = df['full_address'] + ', Sverige'

    return df


def geocode_addresses(df, limit=50):
    """Geocode addresses with rate limiting."""
    geolocator = Nominatim(user_agent="sponsormatch_geocoder")
    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)

    geocoded = []

    for idx, row in df.iterrows():
        if idx >= limit:  # Limit for testing
            break

        try:
            location = geocode(row['full_address'])
            if location:
                geocoded.append({
                    'name': row['Namn'],
                    'address': row['full_address'],
                    'lat': location.latitude,
                    'lon': location.longitude
                })
                logger.info(f"Geocoded: {row['Namn']} -> {location.latitude}, {location.longitude}")
            else:
                # Try with just city name
                location = geocode(f"{row['Postort']}, Sverige")
                if location:
                    geocoded.append({
                        'name': row['Namn'],
                        'address': row['full_address'],
                        'lat': location.latitude,
                        'lon': location.longitude
                    })
                    logger.info(f"Geocoded (city): {row['Namn']} -> {location.latitude}, {location.longitude}")
                else:
                    logger.warning(f"Could not geocode: {row['Namn']}")

        except Exception as e:
            logger.error(f"Geocoding error for {row['Namn']}: {e}")
            time.sleep(2)  # Extra delay on error

    return pd.DataFrame(geocoded)


def assign_size_buckets(df):
    """Assign size buckets based on name patterns and random distribution."""
    import random

    # Keywords that suggest size
    large_keywords = ['IFK', 'GAIS', 'BK Häcken', 'Örgryte']
    small_keywords = ['FF', 'Futsal', 'Ungdom']

    def get_size_bucket(name):
        name_upper = name.upper()

        # Check for large club indicators
        for keyword in large_keywords:
            if keyword.upper() in name_upper:
                return 'large'

        # Check for small club indicators
        for keyword in small_keywords:
            if keyword.upper() in name_upper:
                return 'small'

        # Random distribution for others
        rand = random.random()
        if rand < 0.3:
            return 'small'
        elif rand < 0.7:
            return 'medium'
        else:
            return 'large'

    df['size_bucket'] = df['name'].apply(get_size_bucket)

    # Assign member counts based on size
    def get_member_count(size):
        if size == 'small':
            return random.randint(50, 150)
        elif size == 'medium':
            return random.randint(151, 500)
        else:
            return random.randint(501, 1500)

    df['member_count'] = df['size_bucket'].apply(get_member_count)
    df['founded_year'] = [random.randint(1900, 2020) for _ in range(len(df))]

    return df


def main():
    """Main ingestion function."""
    # Read the CSV
    csv_path = Path("data/gothenburg_associations.csv")

    try:
        # Try different encodings
        for encoding in ['utf-8', 'latin-1', 'iso-8859-1']:
            try:
                df = pd.read_csv(csv_path, encoding=encoding)
                logger.info(f"Successfully read CSV with {encoding} encoding")
                break
            except UnicodeDecodeError:
                continue
    except Exception as e:
        logger.error(f"Could not read CSV: {e}")
        return

    # Filter out empty rows
    df = df[df['Namn'].notna()].copy()
    logger.info(f"Found {len(df)} associations")

    # Clean data
    df = clean_associations_data(df)

    # Geocode addresses (limited for testing)
    logger.info("Starting geocoding...")
    geocoded_df = geocode_addresses(df, limit=30)  # Limit to 30 for testing

    if geocoded_df.empty:
        logger.error("No addresses geocoded")
        return

    # Add size buckets and other fields
    geocoded_df = assign_size_buckets(geocoded_df)

    # Prepare for database
    geocoded_df['id'] = range(1, len(geocoded_df) + 1)

    # Insert into database
    engine = get_engine()

    # Create tables if needed
    Base.metadata.create_all(bind=engine)

    # Clear existing data
    with engine.begin() as conn:
        conn.execute(text("DELETE FROM associations"))

        # Insert new data
        geocoded_df.to_sql(
            'associations',
            conn,
            if_exists='append',
            index=False,
            method='multi'
        )

    logger.info(f"Successfully loaded {len(geocoded_df)} associations")

    # Add some sample data for major clubs if not geocoded
    add_sample_major_clubs(engine)


def add_sample_major_clubs(engine):
    """Add well-known Gothenburg clubs with accurate coordinates."""
    major_clubs = [
        {
            'name': 'IFK Göteborg',
            'address': 'Kamratgårdsvägen 50, 416 55 Göteborg, Sverige',
            'lat': 57.706547,
            'lon': 11.980125,
            'size_bucket': 'large',
            'member_count': 1500,
            'founded_year': 1904
        },
        {
            'name': 'GAIS',
            'address': 'Gamla Boråsvägen 75, 412 76 Göteborg, Sverige',
            'lat': 57.687932,
            'lon': 11.989746,
            'size_bucket': 'large',
            'member_count': 1200,
            'founded_year': 1894
        },
        {
            'name': 'BK Häcken',
            'address': 'Entreprenadvägen 6, 417 05 Göteborg, Sverige',
            'lat': 57.705891,
            'lon': 11.936847,
            'size_bucket': 'large',
            'member_count': 1000,
            'founded_year': 1940
        }
    ]

    # Check if these already exist
    with engine.begin() as conn:
        for club in major_clubs:
            exists = conn.execute(
                text("SELECT 1 FROM associations WHERE name = :name"),
                {"name": club['name']}
            ).first()

            if not exists:
                conn.execute(
                    text("""
                        INSERT INTO associations 
                        (name, address, lat, lon, size_bucket, member_count, founded_year)
                        VALUES (:name, :address, :lat, :lon, :size_bucket, :member_count, :founded_year)
                    """),
                    club
                )
                logger.info(f"Added major club: {club['name']}")


if __name__ == "__main__":
    main()


================================================================================
FIL: sponsor_match/data/ingest_companies.py
================================================================================

# This script has been annotated with comments in British English.
# Detailed comments explaining each section have been added as requested.

#!/usr/bin/env python3
"""
sponsor_match/data/ingest_companies.py
-----------------------------------------
Read data/bolag_1_500_with_coords.csv (already geocoded) and load into companies table.
"""

import logging
import sys
from pathlib import Path

import pandas as pd
from sqlalchemy import text

from sponsor_match.core.db import get_engine

# Configure logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def main() -> None:
    # Use the already geocoded file
    project_root = Path(__file__).resolve().parents[2]
    csv_path = project_root / "data" / "bolag_1_500_with_coords.csv"

    # Read the geocoded file
    try:
        df = pd.read_csv(csv_path, encoding="utf-8")
        logger.info("Loaded %d rows from %s", len(df), csv_path)
    except FileNotFoundError:
        logger.error("CSV not found at %s", csv_path)
        sys.exit(1)
    except Exception as e:
        logger.exception("Unexpected read error: %s", e)
        sys.exit(1)

    # Rename columns to match database schema
    df = df.rename(columns={
        "Företagsnamn": "name",
        "Postadress": "address",
        "Omsättning (tkr)": "revenue_ksek",
        "Anställda": "employees",
        "År": "year",
    })

    # Calculate size bucket based on revenue
    def calculate_size_bucket(revenue_ksek):
        if pd.isna(revenue_ksek):
            return "medium"
        revenue_sek = revenue_ksek * 1000
        if revenue_sek < 5_000_000:
            return "small"
        elif revenue_sek < 50_000_000:
            return "medium"
        else:
            return "large"

    df["size_bucket"] = df["revenue_ksek"].apply(calculate_size_bucket)

    # Add default industry and orgnr to match database schema
    df["industry"] = "Other"
    df["orgnr"] = None  # No organization numbers in our data

    # Select final columns matching database schema
    final_columns = ["orgnr", "name", "revenue_ksek", "employees", "year", "size_bucket", "industry", "lat", "lon"]
    df = df[final_columns]

    # Drop rows with missing coordinates
    before_count = len(df)
    df = df.dropna(subset=["lat", "lon"])
    after_count = len(df)
    if before_count > after_count:
        logger.warning("Dropped %d rows with missing coordinates", before_count - after_count)

    # Write to database
    engine = get_engine()

    try:
        with engine.begin() as conn:
            # Clear existing data and insert new
            conn.execute(text("DELETE FROM companies"))
            df.to_sql("companies", conn, if_exists="append", index=False)

            new_count = conn.execute(text("SELECT COUNT(*) FROM companies")).scalar() or 0
            logger.info("✅ Companies ingestion complete. Total rows: %d", new_count)

    except Exception as e:
        logger.exception("DB error during ingest: %s", e)
        sys.exit(1)


if __name__ == "__main__":
    main()

================================================================================
FIL: utils/__init__.py
================================================================================



================================================================================
FIL: utils/check_db.py
================================================================================

# Utility to verify database connectivity and inspect tables
# Detailed comments have been inserted in British English.

#!/usr/bin/env python3
"""
utils/check_db.py

Utility to verify database connectivity and inspect tables.
"""

# Standard library or third-party import
import logging
# Standard library or third-party import
import sys

# Standard library or third-party import
from dotenv import load_dotenv
# Standard library or third-party import
from sqlalchemy import inspect

# Standard library or third-party import
from sponsor_match.core.db import get_engine


# Definition of function 'main': explains purpose and parameters
def main():
    # Load environment variables (e.g. DATABASE_URL)
    load_dotenv()

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s"
    )

    # Create/get the SQLAlchemy engine
    try:
        engine = get_engine()
    except RuntimeError as e:
        logging.error(f"Could not create database engine: {e}")
        sys.exit(1)

    # Inspect the database
    inspector = inspect(engine)
    tables = inspector.get_table_names()
    if not tables:
        logging.warning("No tables found in the database.")
        return

    logging.info(f"Found tables: {tables}")

    # For each table, count rows
    for table in tables:
        try:
            with engine.connect() as conn:
                result = conn.execute(f"SELECT COUNT(*) FROM {table}")
                count = result.scalar()
            logging.info(f"Table '{table}' has {count} records.")
        except Exception as e:
            logging.error(f"Error querying table '{table}': {e}")

# Entry point check: script execution starts here when run directly
if __name__ == "__main__":
    main()

================================================================================
FIL: utils/generate_sample_data.py
================================================================================

# Module to generate realistic sample data for SponsorMatch AI
# Detailed comments have been inserted in British English.

#!/usr/bin/env python3
"""Generate realistic sample data for SponsorMatch AI."""

# Standard library or third-party import
import random
# Standard library or third-party import
from pathlib import Path

# Standard library or third-party import
import numpy as np
# Standard library or third-party import
import pandas as pd
# Standard library or third-party import
from faker import Faker

# Set seeds for reproducibility
random.seed(42)
np.random.seed(42)
fake = Faker()
Faker.seed(42)


# Definition of function 'generate_sample_companies': explains purpose and parameters
def generate_sample_companies(n=50):
    """Generate realistic Swedish companies."""
    companies = []

    # Swedish company types and their characteristics
    industries = {
        'Technology': {'revenue_range': (10000, 500000), 'employee_range': (10, 200)},
        'Manufacturing': {'revenue_range': (50000, 1000000), 'employee_range': (50, 500)},
        'Finance': {'revenue_range': (25000, 750000), 'employee_range': (20, 300)},
        'Healthcare': {'revenue_range': (15000, 400000), 'employee_range': (15, 150)},
        'Retail': {'revenue_range': (20000, 300000), 'employee_range': (25, 100)},
    }

    # Swedish cities with coordinates
    cities = [
        ('Göteborg', 57.7089, 11.9746),
        ('Stockholm', 59.3293, 18.0686),
        ('Malmö', 55.6050, 13.0038),
        ('Uppsala', 59.8586, 17.6389),
        ('Linköping', 58.4108, 15.6214)
    ]

    for i in range(n):
        industry = random.choice(list(industries.keys()))
        city, base_lat, base_lon = random.choice(cities)

        # Add some randomness to coordinates
        lat = base_lat + random.uniform(-0.1, 0.1)
        lon = base_lon + random.uniform(-0.1, 0.1)

        revenue = random.randint(*industries[industry]['revenue_range'])
        employees = random.randint(*industries[industry]['employee_range'])

        # Determine size bucket
        if revenue < 50000:
            size_bucket = 'small'
        elif revenue < 250000:
            size_bucket = 'medium'
        else:
            size_bucket = 'large'

        company = {
            'id': i + 1,
            'orgnr': f"{random.randint(100000, 999999)}-{random.randint(1000, 9999)}",
            'name': f"{fake.company()} {random.choice(['AB', 'Ltd', 'Group'])}",
            'revenue_ksek': revenue,
            'employees': employees,
            'year': random.randint(2020, 2024),
            'size_bucket': size_bucket,
            'industry': industry,
            'lat': round(lat, 6),
            'lon': round(lon, 6)
        }
        companies.append(company)

    return pd.DataFrame(companies)


# Definition of function 'generate_sample_associations': explains purpose and parameters
def generate_sample_associations(n=30):
    """Generate realistic Swedish sports associations."""
    associations = []

    sports = ['Fotboll', 'Ishockey', 'Bandy', 'Handboll', 'Basket', 'Innebandy']
    cities = [
        ('Göteborg', 57.7089, 11.9746),
        ('Mölndal', 57.6554, 12.0134),
        ('Partille', 57.7394, 12.1065),
        ('Lerum', 57.7706, 12.2694),
        ('Kungälv', 57.8700, 11.9800)
    ]

    for i in range(n):
        sport = random.choice(sports)
        city, base_lat, base_lon = random.choice(cities)

        lat = base_lat + random.uniform(-0.05, 0.05)
        lon = base_lon + random.uniform(-0.05, 0.05)

        members = random.randint(50, 800)

        # Size bucket based on members
        if members < 150:
            size_bucket = 'small'
        elif members < 400:
            size_bucket = 'medium'
        else:
            size_bucket = 'large'

        association = {
            'id': i + 1,
            'name': f"{city} {sport}klub",
            'member_count': members,
            'address': f"{fake.street_address()}, {city}",
            'lat': round(lat, 6),
            'lon': round(lon, 6),
            'size_bucket': size_bucket,
            'founded_year': random.randint(1950, 2020)
        }
        associations.append(association)

    return pd.DataFrame(associations)


# Definition of function 'main': explains purpose and parameters
def main():
    """Generate and save sample data."""
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data"
    data_dir.mkdir(exist_ok=True)

    # Generate data
    print("Generating sample companies...")
    companies = generate_sample_companies(50)
    companies.to_csv(data_dir / "sample_companies.csv", index=False)
    print(f"Generated {len(companies)} companies")

    print("Generating sample associations...")
    associations = generate_sample_associations(30)
    associations.to_csv(data_dir / "sample_associations.csv", index=False)
    print(f"Generated {len(associations)} associations")

    print(f"Data saved to {data_dir}")


# Entry point check: script execution starts here when run directly
if __name__ == "__main__":
    main()

================================================================================
FIL: utils/list_project_files.py
================================================================================

# Utility to list project files and dump their contents
# Detailed comments have been inserted in British English.

#!/usr/bin/env python3
"""
utils/list_project_files.py
----------------------
Recursively scans the entire project root (one level up from this script), skips
directories .venv, .venv312, __pycache__, .git, and archive/old_csv_files/,
and writes out both filenames and entire contents for file types:
.py, .csv, .json, .toml, .md and .yml

Now creates four separate dump files to split the content evenly.

Usage:
    cd /home/user/SponsorMatchAI
    python utils/list_project_files.py [--output-prefix CUSTOM_PREFIX] [--verbose]

Output:
    utils_output/project_dump_part1.txt through utils_output/project_dump_part4.txt (default)
"""

# Standard library or third-party import
import argparse
# Standard library or third-party import
import logging
# Standard library or third-party import
import os
# Standard library or third-party import
from pathlib import Path
# Standard library or third-party import
from typing import List, Set

# Configure logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

# Directory names to exclude (checked against directory name only)
EXCLUDE_DIRS = {'.venv', '.venv312', '__pycache__', '.git'}

# Directory paths to exclude (checked against relative path from project root)
EXCLUDE_DIR_PATHS = {'archive/old_csv_files'}

# Specific files to exclude by their relative path
EXCLUDE_FILES = {
    'data/bolag_1_500_sorted_with_year.csv',
    'data/associations_goteborg.csv',
    'data/associations_goteborg_with_coords.csv',
    'data/gothenburg_companies_addresses.csv',
    'data/associations_geocoded.csv',
    'data/companies_geocoded.csv',
    'data/gothenburg_associations.csv',
    'data/municipality_of_goteborg.csv',
    'data/associations_geocoded_prepared.csv',
    'data/companies_geocoded.csv',
    'associations_prepared.csv',
    'data/companies_prepared.csv',
    'data/associations_prepared.csv',
}

# Filenames to exclude regardless of their location
EXCLUDE_FILENAMES = {'bolag_1_500_with_coords.csv'}


# Definition of function 'collect_files': explains purpose and parameters
def collect_files(root: Path, exts: List[str], exclude_files: Set[str], exclude_filenames: Set[str]) -> List[Path]:
    """
    Go through root and all subdirectories, collect files with suffixes in exts,
    skip EXCLUDE_DIRS (by directory name), EXCLUDE_DIR_PATHS (by relative path),
    exclude_files, and exclude_filenames, and return a list of file paths.
    """
    collected_files = []
    excluded_count = 0

    for dirpath, dirnames, filenames in os.walk(root):
        # Get relative path from root
        rel_dirpath = Path(dirpath).relative_to(root)

        # Skip if this directory is in an excluded path
        skip_dir = False
        for exclude_path in EXCLUDE_DIR_PATHS:
            if exclude_path in str(rel_dirpath):
                logger.debug(f"Skipping directory: {rel_dirpath}")
                skip_dir = True
                break

        if skip_dir:
            dirnames[:] = []  # Don't descend into subdirectories
            continue

        # Exclude unwanted directories by name
        dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS]

        for fn in sorted(filenames):
            # Skip files with excluded filenames
            if fn in exclude_filenames:
                logger.debug(f"Excluding file by name: {fn} in {dirpath}")
                excluded_count += 1
                continue

            if any(fn.lower().endswith(ext) for ext in exts):
                file_path = Path(dirpath) / fn
                rel_path = str(file_path.relative_to(root))

                # Skip excluded files by path
                if rel_path in exclude_files:
                    logger.debug(f"Excluding file by path: {rel_path}")
                    excluded_count += 1
                    continue

                collected_files.append(file_path)

    # Log summary of exclusions only
    if excluded_count > 0:
        logger.info(f"Excluded {excluded_count} files based on exclusion rules")

    return collected_files


# Definition of function 'write_files_to_dump': explains purpose and parameters
def write_files_to_dump(files: List[Path], root: Path, out_path: Path) -> None:
    """
    Write the given files to the output dump file.
    """
    with out_path.open("w", encoding="utf-8") as f:
        for file_path in files:
            f.write("=" * 80 + "\n")
            f.write(f"FIL: {file_path.relative_to(root)}\n")
            f.write("=" * 80 + "\n\n")
            try:
                content = file_path.read_text(encoding="utf-8")
                f.write(content)
            except Exception as e:
                f.write(f"<Could not read file: {e}>\n")
            f.write("\n\n")
    size_kb = out_path.stat().st_size / 1024
    logger.info("Created dump file %s (%.1f KB)", out_path, size_kb)


# Definition of function 'split_into_four_parts': explains purpose and parameters
def split_into_four_parts(files: List[Path]) -> List[List[Path]]:
    """
    Split the list of files into four approximately equal parts.
    """
    file_count = len(files)
    quarter = file_count // 4

    # Handle case where file count doesn't divide evenly by 4
    # by distributing remainder to first parts
    remainder = file_count % 4

    # Calculate lengths for each part
    lengths = [quarter + (1 if i < remainder else 0) for i in range(4)]

    # Create the parts
    result = []
    start_idx = 0
    for length in lengths:
        end_idx = start_idx + length
        result.append(files[start_idx:end_idx])
        start_idx = end_idx

    return result


# Definition of function 'main': explains purpose and parameters
def main() -> None:
    parser = argparse.ArgumentParser(
        description="Create four dump files of important project files with full content."
    )
    parser.add_argument(
        "--output-prefix", "-o",
        type=str,
        default=None,
        help="Output file prefix (default: utils_output/project_dump)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Show detailed exclusion messages"
    )
    args = parser.parse_args()

    # Set logging level based on verbose flag
    if args.verbose:
        logger.setLevel(logging.DEBUG)
        # Also set root logger to DEBUG for full verbosity
        logging.getLogger().setLevel(logging.DEBUG)

    # Since we're now in utils/ directory, need to get the project root (one level up)
    project_root = Path(__file__).parent.parent

    # Create utils_output directory if it doesn't exist
    outputs_dir = project_root / "utils_output"
    outputs_dir.mkdir(exist_ok=True)

    # Set default output prefix if not specified
    output_prefix = args.output_prefix if args.output_prefix else outputs_dir / "project_dump"

    extensions = [".py", ".csv", ".json", ".toml", ".md", ".yml"]
    logger.info("Scanning %s for %s files...", project_root, extensions)

    # Collect all relevant files, excluding specific files
    all_files = collect_files(project_root, extensions, EXCLUDE_FILES, EXCLUDE_FILENAMES)
    logger.info("Found %d files to include in dumps", len(all_files))

    # Split files into four groups
    file_parts = split_into_four_parts(all_files)

    # Write each group to a separate dump file
    for i, files_part in enumerate(file_parts, 1):
        output_path = Path(f"{output_prefix}_part{i}.txt")
        write_files_to_dump(files_part, project_root, output_path)
        logger.info("Part %d contains %d files", i, len(files_part))

    logger.info("Created four dump files with prefix: %s", output_prefix)


# Entry point check: script execution starts here when run directly
if __name__ == "__main__":
    main()

================================================================================
FIL: utils/setup_database.py
================================================================================

# Script to set up database tables and load sample data
# Detailed comments have been inserted in British English.

#!/usr/bin/env python3
"""
setup_database.py - Complete database setup for SponsorMatch AI
Ensures both associations and companies are properly loaded.
"""

# Standard library or third-party import
import logging

# Standard library or third-party import
import pandas as pd
# Standard library or third-party import
from sqlalchemy import text

# Standard library or third-party import
from sponsor_match.core.db import get_engine
# Standard library or third-party import
from sponsor_match.models.entities import Base

# Standard library or third-party import

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Definition of function 'setup_database': explains purpose and parameters
def setup_database():
    """Set up the complete database with sample data."""
    engine = get_engine()

    # Create all tables
    Base.metadata.create_all(bind=engine)
    logger.info("Created database tables")

    # Load associations
    load_sample_associations(engine)

    # Load companies
    load_sample_companies(engine)

    # Verify data
    verify_data(engine)


# Definition of function 'load_sample_associations': explains purpose and parameters
def load_sample_associations(engine):
    """Load sample associations with realistic Gothenburg data."""
    associations = [
        # Large clubs
        {'name': 'IFK Göteborg', 'address': 'Kamratgårdsvägen 50, 416 55 Göteborg',
         'lat': 57.706547, 'lon': 11.980125, 'size_bucket': 'large', 'member_count': 1500, 'founded_year': 1904},
        {'name': 'GAIS', 'address': 'Gamla Boråsvägen 75, 412 76 Göteborg',
         'lat': 57.687932, 'lon': 11.989746, 'size_bucket': 'large', 'member_count': 1200, 'founded_year': 1894},
        {'name': 'BK Häcken', 'address': 'Entreprenadvägen 6, 417 05 Göteborg',
         'lat': 57.705891, 'lon': 11.936847, 'size_bucket': 'large', 'member_count': 1000, 'founded_year': 1940},
        {'name': 'Örgryte IS', 'address': 'Skånegatan 5, 412 51 Göteborg',
         'lat': 57.693734, 'lon': 11.996542, 'size_bucket': 'large', 'member_count': 900, 'founded_year': 1887},

        # Medium clubs
        {'name': 'Qviding FIF', 'address': 'Härlanda Park 6B, 416 52 Göteborg',
         'lat': 57.703456, 'lon': 12.015234, 'size_bucket': 'medium', 'member_count': 400, 'founded_year': 1987},
        {'name': 'Västra Frölunda IF', 'address': 'Klubbvägen 19, 421 47 Västra Frölunda',
         'lat': 57.652341, 'lon': 11.928765, 'size_bucket': 'medium', 'member_count': 350, 'founded_year': 1947},
        {'name': 'Kärra KIF', 'address': 'Burmans gata 3, 425 33 Hisings Kärra',
         'lat': 57.765432, 'lon': 11.945678, 'size_bucket': 'medium', 'member_count': 300, 'founded_year': 1970},
        {'name': 'Sävedalens IF', 'address': 'Hultvägen 2, 433 64 Sävedalen',
         'lat': 57.709876, 'lon': 12.057891, 'size_bucket': 'medium', 'member_count': 450, 'founded_year': 1948},

        # Small clubs
        {'name': 'Majorna BK', 'address': 'Karl Johansgatan 152, 414 51 Göteborg',
         'lat': 57.689012, 'lon': 11.914567, 'size_bucket': 'small', 'member_count': 120, 'founded_year': 1990},
        {'name': 'Lundby IF', 'address': 'Munkedalsgatan 10, 417 16 Göteborg',
         'lat': 57.719234, 'lon': 11.942345, 'size_bucket': 'small', 'member_count': 100, 'founded_year': 2005},
        {'name': 'Gamlestaden FF', 'address': 'Artillerigatan 33, 415 02 Göteborg',
         'lat': 57.725678, 'lon': 12.005432, 'size_bucket': 'small', 'member_count': 80, 'founded_year': 2010},
        {'name': 'Kortedala IK', 'address': 'Julaftonsgatan 58, 415 44 Göteborg',
         'lat': 57.737890, 'lon': 12.026789, 'size_bucket': 'small', 'member_count': 90, 'founded_year': 1969},
    ]

    df = pd.DataFrame(associations)

    with engine.begin() as conn:
        # Clear existing data
        conn.execute(text("DELETE FROM associations"))

        # Insert new data
        df.to_sql('associations', conn, if_exists='append', index=False)

    logger.info(f"Loaded {len(df)} associations")


# Definition of function 'load_sample_companies': explains purpose and parameters
def load_sample_companies(engine):
    """Load sample companies in Gothenburg area."""
    companies = []

    # Technology companies (10-char orgnr format)
    companies.extend([
        {'name': 'Volvo Tech AB', 'orgnr': '5560123456', 'revenue_ksek': 150000, 'employees': 200,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Technology', 'lat': 57.708765, 'lon': 11.965432},
        {'name': 'Ericsson Göteborg', 'orgnr': '5560234567', 'revenue_ksek': 200000, 'employees': 300,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Technology', 'lat': 57.695432, 'lon': 11.987654},
        {'name': 'IT Solutions AB', 'orgnr': '5560345678', 'revenue_ksek': 25000, 'employees': 50,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Technology', 'lat': 57.712345, 'lon': 11.998765},
    ])

    # Finance companies
    companies.extend([
        {'name': 'Nordea Göteborg', 'orgnr': '5560456789', 'revenue_ksek': 300000, 'employees': 150,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Finance', 'lat': 57.701234, 'lon': 11.975432},
        {'name': 'SEB Private Banking', 'orgnr': '5560567890', 'revenue_ksek': 180000, 'employees': 80,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Finance', 'lat': 57.698765, 'lon': 11.968901},
        {'name': 'Finanskonsult AB', 'orgnr': '5560678901', 'revenue_ksek': 15000, 'employees': 20,
         'year': 2023, 'size_bucket': 'small', 'industry': 'Finance', 'lat': 57.715678, 'lon': 11.945678},
    ])

    # Manufacturing companies
    companies.extend([
        {'name': 'SKF Sverige AB', 'orgnr': '5560789012', 'revenue_ksek': 500000, 'employees': 1000,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Manufacturing', 'lat': 57.721234, 'lon': 11.890123},
        {'name': 'Göteborg Mekaniska', 'orgnr': '5560890123', 'revenue_ksek': 45000, 'employees': 100,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Manufacturing', 'lat': 57.735678, 'lon': 11.912345},
        {'name': 'Precision Tools AB', 'orgnr': '5560901234', 'revenue_ksek': 30000, 'employees': 60,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Manufacturing', 'lat': 57.745678, 'lon': 11.923456},
    ])

    # Retail companies
    companies.extend([
        {'name': 'ICA Maxi Göteborg', 'orgnr': '5561012345', 'revenue_ksek': 120000, 'employees': 150,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Retail', 'lat': 57.689012, 'lon': 11.934567},
        {'name': 'Systembolaget City', 'orgnr': '5561123456', 'revenue_ksek': 80000, 'employees': 50,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Retail', 'lat': 57.705678, 'lon': 11.967890},
        {'name': 'Sportbutiken AB', 'orgnr': '5561234567', 'revenue_ksek': 18000, 'employees': 25,
         'year': 2023, 'size_bucket': 'small', 'industry': 'Retail', 'lat': 57.698765, 'lon': 11.956789},
    ])

    # Healthcare companies
    companies.extend([
        {'name': 'Sahlgrenska Life', 'orgnr': '5561345678', 'revenue_ksek': 250000, 'employees': 400,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Healthcare', 'lat': 57.683456, 'lon': 11.962345},
        {'name': 'Vårdcentral Väst', 'orgnr': '5561456789', 'revenue_ksek': 35000, 'employees': 45,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Healthcare', 'lat': 57.654321, 'lon': 11.923456},
        {'name': 'Hälsokliniken', 'orgnr': '5561567890', 'revenue_ksek': 12000, 'employees': 15,
         'year': 2023, 'size_bucket': 'small', 'industry': 'Healthcare', 'lat': 57.712345, 'lon': 11.989012},
    ])

    # Add more companies spread around Gothenburg
# Standard library or third-party import
    import random
    for i in range(20):
        lat = 57.65 + random.uniform(0, 0.1)
        lon = 11.9 + random.uniform(0, 0.15)
        revenue = random.randint(5000, 100000)

        size = 'small' if revenue < 20000 else 'medium' if revenue < 100000 else 'large'

        companies.append({
            'name': f'Local Business {i + 1} AB',
            'orgnr': f'556{200 + i:03d}{1000 + i:04d}',  # Format: 10 digits, no hyphen
            'revenue_ksek': revenue,
            'employees': random.randint(5, 200),
            'year': 2023,
            'size_bucket': size,
            'industry': random.choice(['Technology', 'Finance', 'Manufacturing', 'Retail', 'Healthcare']),
            'lat': lat,
            'lon': lon
        })

    df = pd.DataFrame(companies)

    with engine.begin() as conn:
        # Clear existing data
        conn.execute(text("DELETE FROM companies"))

        # Insert new data
        df.to_sql('companies', conn, if_exists='append', index=False)

    logger.info(f"Loaded {len(df)} companies")


# Definition of function 'verify_data': explains purpose and parameters
def verify_data(engine):
    """Verify the loaded data."""
    with engine.connect() as conn:
        # Count associations
        assoc_count = conn.execute(text("SELECT COUNT(*) FROM associations")).scalar()
        logger.info(f"Total associations: {assoc_count}")

        # Count by size bucket
        size_counts = conn.execute(text("""
            SELECT size_bucket, COUNT(*) as count 
            FROM associations 
            GROUP BY size_bucket
        """)).fetchall()

        for size, count in size_counts:
            logger.info(f"  {size}: {count} associations")

        # Count companies
        comp_count = conn.execute(text("SELECT COUNT(*) FROM companies")).scalar()
        logger.info(f"Total companies: {comp_count}")

        # Count by industry
        industry_counts = conn.execute(text("""
            SELECT industry, COUNT(*) as count 
            FROM companies 
            GROUP BY industry
        """)).fetchall()

        for industry, count in industry_counts:
            logger.info(f"  {industry}: {count} companies")


# Entry point check: script execution starts here when run directly
if __name__ == "__main__":
    setup_database()

================================================================================
FIL: utils/train_clustering_models.py
================================================================================

# Train clustering models for associations and companies
# Detailed comments have been inserted in British English.

#!/usr/bin/env python3
"""
train_clustering_models.py - Train clustering models with consistent features
"""

# Standard library or third-party import
import logging
# Standard library or third-party import
from pathlib import Path

# Standard library or third-party import
import joblib
# Standard library or third-party import
import numpy as np
# Standard library or third-party import
import pandas as pd
# Standard library or third-party import
from sklearn.cluster import KMeans
# Standard library or third-party import
from sklearn.preprocessing import StandardScaler

# Standard library or third-party import
from sponsor_match.core.db import get_engine

# Standard library or third-party import

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ensure models directory exists
MODELS_DIR = Path(__file__).parent.parent / "models"
MODELS_DIR.mkdir(exist_ok=True)

# Feature configuration
FEATURES = ['lat', 'lon']  # Use only lat/lon for consistent features
N_CLUSTERS = {
    'default': 5,
    'large': 3
}


# Definition of function 'load_data': explains purpose and parameters
def load_data():
    """Load associations and companies from database."""
    engine = get_engine()

    with engine.connect() as conn:
        # Load associations
        associations = pd.read_sql("""
            SELECT id, name, lat, lon, size_bucket, member_count
            FROM associations
            WHERE lat IS NOT NULL AND lon IS NOT NULL
        """, conn)

        # Load companies
        companies = pd.read_sql("""
            SELECT id, name, lat, lon, size_bucket, revenue_ksek
            FROM companies
            WHERE lat IS NOT NULL AND lon IS NOT NULL
        """, conn)

    return associations, companies


# Definition of function 'train_clustering_model': explains purpose and parameters
def train_clustering_model(data, n_clusters, model_name):
    """Train a clustering model on the given data."""
    # Extract features
    X = data[FEATURES].values

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Train KMeans
    kmeans = KMeans(n_clusters=min(n_clusters, len(data)), random_state=42)
    kmeans.fit(X_scaled)

    # Create model package with scaler
    model_package = {
        'scaler': scaler,
        'kmeans': kmeans,
        'features': FEATURES,
        'n_features': len(FEATURES)
    }

    # Save model
    model_path = MODELS_DIR / f"{model_name}.joblib"
    joblib.dump(model_package, model_path)
    logger.info(f"Saved {model_name} model to {model_path}")

    # Report cluster sizes
    labels = kmeans.labels_
    unique, counts = np.unique(labels, return_counts=True)
    for label, count in zip(unique, counts):
        logger.info(f"  Cluster {label}: {count} items")

    return model_package


# Definition of function 'train_all_models': explains purpose and parameters
def train_all_models():
    """Train all clustering models."""
    # Load data
    associations, companies = load_data()

    if associations.empty or companies.empty:
        logger.error("No data found. Run setup_database.py first.")
        return

    logger.info(f"Loaded {len(associations)} associations and {len(companies)} companies")

    # Combine all data for default model
    all_data = pd.concat([
        associations[FEATURES + ['size_bucket']],
        companies[FEATURES + ['size_bucket']]
    ], ignore_index=True)

    # Train default model (for small/medium entities)
    default_data = all_data[all_data['size_bucket'].isin(['small', 'medium'])]
    if len(default_data) > 0:
        logger.info(f"\nTraining default model with {len(default_data)} entities")
        train_clustering_model(default_data, N_CLUSTERS['default'], 'kmeans')

    # Train large model (for large entities)
    large_data = all_data[all_data['size_bucket'] == 'large']
    if len(large_data) > 0:
        logger.info(f"\nTraining large model with {len(large_data)} entities")
        train_clustering_model(large_data, N_CLUSTERS['large'], 'kmeans_large')

    # Create simplified models for backward compatibility
    create_backward_compatible_models()


# Definition of function 'create_backward_compatible_models': explains purpose and parameters
def create_backward_compatible_models():
    """Create simplified models for backward compatibility."""
    # Load the model packages
    default_package = joblib.load(MODELS_DIR / "kmeans.joblib")
    large_package = joblib.load(MODELS_DIR / "kmeans_large.joblib")

    # Extract just the KMeans models
    joblib.dump(default_package['kmeans'], MODELS_DIR / "kmeans_simple.joblib")
    joblib.dump(large_package['kmeans'], MODELS_DIR / "kmeans_large_simple.joblib")

    logger.info("Created backward compatible models")


# Definition of function 'test_models': explains purpose and parameters
def test_models():
    """Test the trained models."""
    # Load a model
    model_package = joblib.load(MODELS_DIR / "kmeans.joblib")
    scaler = model_package['scaler']
    kmeans = model_package['kmeans']

    # Test points in Gothenburg area
    test_points = [
        [57.7089, 11.9746],  # Central Gothenburg
        [57.6523, 11.9118],  # Västra Frölunda
        [57.7654, 11.9457],  # Hisings Kärra
    ]

    logger.info("\nTesting model predictions:")
    for point in test_points:
        scaled_point = scaler.transform([point])
        cluster = kmeans.predict(scaled_point)[0]
        logger.info(f"  Point {point} -> Cluster {cluster}")


# Entry point check: script execution starts here when run directly
if __name__ == "__main__":
    train_all_models()
    test_models()

================================================================================
FIL: .pytest_cache/README.md
================================================================================

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


================================================================================
FIL: data/examine.py
================================================================================

# examine.py
"""
Analyse failures in geocoded company addresses to identify common patterns.
"""
import pandas as pd  # Data analysis

# Load geocoded companies CSV from current directory
df = pd.read_csv('companies_geocoded.csv')

# Filter entries marked as 'failed' in geocoding confidence
failed = df[df['geocoding_confidence'] == 'failed']

# Show most frequent failed addresses for manual inspection
print("Common patterns in failed addresses:")
print(failed['registered_address'].value_counts().head(20))

# Summary statistics on failure rate
total_failures = len(failed)
print(f"\nTotal failed addresses: {total_failures}")
print(f"Percentage of total: {total_failures / len(df) * 100:.1f}%")

# Group failures by district to see geographic trends
print("\nFailed geocoding by district:")
print(failed['district'].value_counts().head(10))

# Identify common terms in failed addresses (length > 3 characters)
common_terms = {}
for address in failed['registered_address'].dropna():
    for word in address.lower().split():
        if len(word) > 3:
            common_terms[word] = common_terms.get(word, 0) + 1

print("\nCommon terms in failed addresses:")
for term, count in sorted(common_terms.items(), key=lambda x: x[1], reverse=True)[:15]:
    print(f"  '{term}': {count} occurrences")


