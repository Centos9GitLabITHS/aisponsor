================================================================================
FIL: sponsor_match/models/__init__.py
================================================================================



================================================================================
FIL: sponsor_match/models/club_extended.py
================================================================================

#!/usr/bin/env python3
"""
models/club_extended.py
------------------------
Extended data model for sports clubs, including enrichment fields
for membership, financials, and sponsorship details.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional


@dataclass
class ExtendedClub:
    """
    Represents a sports club with both basic and enriched metadata.
    """

    # Basic info
    id: int
    name: str
    member_count: int
    address: str
    lat: Optional[float]
    lon: Optional[float]
    size_bucket: str

    # Extended basic info
    founded_year: int
    club_type: str
    registration_number: str
    website: str
    email: str
    phone: str
    social_media: Dict[str, str] = field(default_factory=dict)

    # Sports & activities
    sport_types: List[str] = field(default_factory=list)
    primary_sport: str = ""
    leagues: List[str] = field(default_factory=list)
    division_level: int = 0

    # Membership breakdown
    active_members: int = 0
    youth_members: int = 0
    gender_distribution: Dict[str, float] = field(default_factory=dict)
    membership_growth_rate: float = 0.0

    # Financials
    annual_revenue: float = 0.0
    sponsorship_revenue: float = 0.0
    financial_status: str = ""

    # Sponsorship history
    current_sponsors: List[str] = field(default_factory=list)
    sponsorship_packages: List[Dict[str, any]] = field(default_factory=list)
    sponsor_retention_rate: float = 0.0

    # Community engagement
    volunteer_count: int = 0
    fan_base_size: int = 0
    social_media_followers: Dict[str, int] = field(default_factory=dict)

    # Infrastructure
    owned_facilities: List[str] = field(default_factory=list)
    stadium_capacity: int = 0
    facility_conditions: Dict[str, str] = field(default_factory=dict)


================================================================================
FIL: sponsor_match/models/clustering.py
================================================================================

#!/usr/bin/env python3
"""
sponsor_match/models/clustering.py

Defines clustering logic for SponsorMatch AI: training, saving, loading, and inference.
"""
import os
import pickle
import logging
from pathlib import Path

import pandas as pd
from sklearn.cluster import KMeans

from sponsor_match.core.config import DATA_DIR, MODELS_DIR, LOG_LEVEL

# Configure logging
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s %(levelname)s %(message)s"
)

# Constants
MODEL_FILE = MODELS_DIR / os.getenv("CLUSTER_MODEL_FILE", "kmeans.pkl")
N_CLUSTERS = int(os.getenv("N_CLUSTERS", 5))
RANDOM_STATE = int(os.getenv("CLUSTER_RANDOM_STATE", 42))
FEATURE_COLUMNS = ["latitude", "longitude"]


def train(
    input_csv: Path = None,
    model_file: Path = None,
    n_clusters: int = N_CLUSTERS,
    random_state: int = RANDOM_STATE,
):
    """
    Train a KMeans clustering model on the association data and save the model to disk.
    """
    if input_csv is None:
        input_csv = DATA_DIR / "associations_goteborg_with_coords.csv"
    if model_file is None:
        model_file = MODEL_FILE

    if not Path(input_csv).exists():
        logging.error(f"Input CSV not found: {input_csv}")
        return

    # Load data
    df = pd.read_csv(input_csv)
    if not all(col in df.columns for col in FEATURE_COLUMNS):
        logging.error(f"Required columns not found in CSV: {FEATURE_COLUMNS}")
        return
    features = df[FEATURE_COLUMNS].dropna()
    if features.empty:
        logging.error("No valid feature data available for clustering.")
        return

    # Train model
    model = KMeans(n_clusters=n_clusters, random_state=random_state)
    model.fit(features)

    # Ensure models directory exists
    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    # Save model
    with open(model_file, "wb") as f:
        pickle.dump(model, f)
    logging.info(f"Trained KMeans ({n_clusters} clusters) and saved to {model_file}")


def load_model(model_file: Path = None):
    """
    Load the clustering model from disk.
    """
    if model_file is None:
        model_file = MODEL_FILE
    if not model_file.exists():
        raise FileNotFoundError(f"Model file not found: {model_file}")
    with open(model_file, "rb") as f:
        model = pickle.load(f)
    return model


def predict(lat: float, lon: float, model=None):
    """
    Predict the cluster label for a given latitude and longitude.
    """
    if model is None:
        model = load_model()
    cluster = model.predict([[lat, lon]])
    return int(cluster[0])


if __name__ == "__main__":
    from argparse import ArgumentParser

    parser = ArgumentParser(description="Train or retrain the clustering model.")
    parser.add_argument("--input-csv", type=Path, help="Path to associations CSV")
    parser.add_argument("--output-model", type=Path, help="Path to save trained model")
    parser.add_argument("--n-clusters", type=int, default=N_CLUSTERS, help="Number of clusters")
    parser.add_argument("--random-state", type=int, default=RANDOM_STATE, help="Random seed")
    args = parser.parse_args()

    train(
        input_csv=args.input_csv,
        model_file=args.output_model,
        n_clusters=args.n_clusters,
        random_state=args.random_state,
    )


================================================================================
FIL: sponsor_match/models/entities.py
================================================================================

#!/usr/bin/env python3
"""
models/entities.py
------------------
Domain entity classes for SponsorMatch AI with proper SQLAlchemy ORM mappings.
"""

from sqlalchemy import Column, Integer, String, Float, Enum
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()


class Association(Base):
    """
    Represents a sports club/association as stored in the `associations` table.
    """
    __tablename__ = 'associations'

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(120))
    member_count = Column(Integer)
    address = Column(String(255))
    lat = Column(Float)
    lon = Column(Float)
    size_bucket = Column(Enum('small', 'medium', 'large'))
    founded_year = Column(Integer)


class Company(Base):
    """
    Represents a company as stored in the `companies` table.
    """
    __tablename__ = 'companies'

    id = Column(Integer, primary_key=True, autoincrement=True)
    orgnr = Column(String(10))
    name = Column(String(200))
    revenue_ksek = Column(Float)
    employees = Column(Integer)
    year = Column(Integer)
    size_bucket = Column(Enum('small', 'medium', 'large'))
    industry = Column(String(120))
    lat = Column(Float)
    lon = Column(Float)


# Legacy dataclasses for backward compatibility
from dataclasses import dataclass
from typing import Optional


@dataclass
class Club:
    """
    Legacy dataclass representation of a sports club.
    Use Association ORM model for database operations.
    """
    id: int
    name: str
    member_count: int
    address: str
    lat: Optional[float]
    lon: Optional[float]
    size_bucket: str
    founded_year: int


@dataclass
class CompanyData:
    """
    Legacy dataclass representation of a company.
    Use Company ORM model for database operations.
    """
    id: int
    orgnr: str
    name: str
    revenue_ksek: float
    employees: int
    year: int
    size_bucket: str
    lat: Optional[float]
    lon: Optional[float]
    industry: str

================================================================================
FIL: sponsor_match/models/features.py
================================================================================

#!/usr/bin/env python3
"""
sponsor_match/models/features.py
--------------------------------
Feature engineering for SponsorMatch ML models.

This module provides comprehensive feature engineering capabilities for analyzing
and scoring potential sponsor-club matches based on multiple dimensions including
geographic proximity, size compatibility, and industry relevance.
"""

import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict

from geopy.distance import geodesic


class FeatureEngineer:
    """
    Compute pairwise features between clubs and companies for recommendation ranking.

    This class contains methods for calculating various features used in matching
    sponsors with clubs, including geographic distance, size compatibility, industry
    affinity, and economic indicators.
    """

    @staticmethod
    def calculate_distance_km(
        lat1: float, lon1: float, lat2: float, lon2: float
    ) -> float:
        """
        Calculate the geodesic distance in kilometers between two latitude/longitude points.

        Parameters
        ----------
        lat1, lon1 : float
            Latitude and longitude of the first point.
        lat2, lon2 : float
            Latitude and longitude of the second point.

        Returns
        -------
        float
            Distance in kilometers.

        Raises
        ------
        TypeError
            If any coordinates are None.
        ValueError
            If any coordinates are zero or negative, or otherwise invalid.
        """
        # 1) None check
        if any(x is None for x in (lat1, lon1, lat2, lon2)):
            raise TypeError("Coordinates cannot be None")

        # 2) Domain check: all lats/lons must be positive in our context
        if any(v <= 0 for v in (lat1, lon1, lat2, lon2)):
            raise ValueError(f"Invalid coordinates for distance calculation: {(lat1, lon1, lat2, lon2)}")

        try:
            return geodesic((lat1, lon1), (lat2, lon2)).km
        except (ValueError, TypeError) as e:
            # Re-raise with more descriptive message
            raise type(e)(f"Invalid coordinates for distance calculation: {e}")

    @staticmethod
    def add_distance(
        df: pd.DataFrame,
        lat: float,
        lon: float,
        lat_col: str = "lat",
        lon_col: str = "lon",
        new_col: str = "distance_km",
    ) -> pd.DataFrame:
        """
        Return a copy of `df` with a new column `new_col` representing the distance
        from the fixed point (`lat`, `lon`) to each row's (lat_col, lon_col).
        """
        df_copy = df.copy()
        df_copy[new_col] = df_copy.apply(
            lambda row: FeatureEngineer.calculate_distance_km(
                lat, lon, row[lat_col], row[lon_col]
            ),
            axis=1,
        )
        return df_copy

    @staticmethod
    def bucket_assoc_size(members: int) -> str:
        """
        Bucket a club's member count into 'small', 'medium', or 'large'.
        """
        if members < 200:
            return "small"
        if members < 1000:
            return "medium"
        return "large"

    @staticmethod
    def calculate_distance(
        club_coords: pd.DataFrame,
        comp_coords: pd.DataFrame
    ) -> pd.Series:
        """
        Compute geodesic distance (km) between each club–company pair (row-wise).
        """
        if len(club_coords) != len(comp_coords):
            raise ValueError("Input DataFrames must have the same length")

        distances = [
            FeatureEngineer.calculate_distance_km(
                club_coords.iloc[i]["lat"],
                club_coords.iloc[i]["lon"],
                comp_coords.iloc[i]["lat"],
                comp_coords.iloc[i]["lon"],
            )
            for i in range(len(club_coords))
        ]
        return pd.Series(distances, name="distance_km")

    @staticmethod
    def calculate_size_match(
        club_sizes: pd.Series,
        comp_sizes: pd.Series
    ) -> pd.Series:
        """
        Score size compatibility: exact match → 1.0; adjacent → 0.5; else → 0.0.
        """
        size_map = {"small": 0, "medium": 1, "large": 2}

        def _score(cs, ps):
            a = size_map.get(cs, 0)
            b = size_map.get(ps, 0)
            if a == b:
                return 1.0
            if abs(a - b) == 1:
                return 0.5
            return 0.0

        scores = [_score(c, p) for c, p in zip(club_sizes, comp_sizes)]
        return pd.Series(scores, name="size_match")

    @staticmethod
    def calculate_industry_affinity(
        sport_types: pd.Series,
        industries: pd.Series
    ) -> pd.Series:
        """
        Calculate industry-sport affinity score (0.0 or 1.0).
        """
        def _affinity(sports, industry):
            if not isinstance(sports, list) or not isinstance(industry, str):
                return 0.0
            return 1.0 if any(sp.lower() in industry.lower() for sp in sports) else 0.0

        affinities = [_affinity(s, i) for s, i in zip(sport_types, industries)]
        return pd.Series(affinities, name="industry_sport_affinity")

    @staticmethod
    def calculate_growth_rate(
        companies_df: pd.DataFrame
    ) -> pd.Series:
        """
        Placeholder for company growth rate; returns zeros until time-series data is available.
        """
        return pd.Series(0.0, index=companies_df.index, name="growth_rate")

    @staticmethod
    def urban_rural_compatibility(
        club_loc: pd.Series,
        comp_loc: pd.Series
    ) -> pd.Series:
        """
        Binary match if club and company share the same location_type.
        """
        compat = [1.0 if cl == cp else 0.0 for cl, cp in zip(club_loc, comp_loc)]
        return pd.Series(compat, name="urban_rural_match")

    @classmethod
    def make_pair_features(cls, df: pd.DataFrame) -> pd.DataFrame:
        """
        Create a feature DataFrame for club–company pairs for the matching model.

        Returns features:
          - distance_km: exact geodesic distance
          - size_match: size compatibility score
          - revenue_ksek: raw revenue value (in tkr)
          - employees: raw employee count
          - distance_score: exp(-distance_km/50) decay
        """
        features: Dict[str, pd.Series] = {}

        # Distance
        if all(col in df.columns for col in ["club_lat", "club_lon", "company_lat", "company_lon"]):
            features["distance_km"] = df.apply(
                lambda r: cls.calculate_distance_km(
                    r["club_lat"], r["club_lon"],
                    r["company_lat"], r["company_lon"]
                ),
                axis=1
            )

        # Size compatibility
        if "club_size" in df.columns and "company_size" in df.columns:
            features["size_match"] = cls.calculate_size_match(
                df["club_size"], df["company_size"]
            )

        # Raw financial and headcount features
        if "revenue_ksek" in df.columns:
            features["revenue_ksek"] = df["revenue_ksek"].rename("revenue_ksek")
        if "employees" in df.columns:
            features["employees"] = df["employees"].rename("employees")

        # Exponential distance decay
        if "distance_km" in features:
            features["distance_score"] = np.exp(-features["distance_km"] / 50)

        return pd.DataFrame(features)

    def create_features(
        self,
        clubs_df: pd.DataFrame,
        companies_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Build a comprehensive feature set for each club–company pair.

        Returns features including:
          - distance_km, distance_score
          - size_match
          - industry_sport_affinity
          - revenue_ksek, employees
          - company_age
          - growth_rate
          - urban_rural_match
        """
        feats: Dict[str, pd.Series] = {}

        # 1) Distance & decay
        if {"lat", "lon"}.issubset(clubs_df.columns) and {"lat", "lon"}.issubset(companies_df.columns):
            club_coords = clubs_df[["lat", "lon"]]
            comp_coords = companies_df[["lat", "lon"]]
            feats["distance_km"] = self.calculate_distance(club_coords, comp_coords)
            feats["distance_score"] = np.exp(-feats["distance_km"] / 50)

        # 2) Size match
        if "size_bucket" in clubs_df.columns and "size_bucket" in companies_df.columns:
            feats["size_match"] = self.calculate_size_match(
                clubs_df["size_bucket"], companies_df["size_bucket"]
            )

        # 3) Industry affinity
        if "sport_types" in clubs_df.columns and "industry" in companies_df.columns:
            feats["industry_sport_affinity"] = self.calculate_industry_affinity(
                clubs_df["sport_types"], companies_df["industry"]
            )

        # 4) Raw financial & headcount
        if "revenue_ksek" in companies_df.columns:
            feats["revenue_ksek"] = companies_df["revenue_ksek"].rename("revenue_ksek")
        if "employees" in companies_df.columns:
            feats["employees"] = companies_df["employees"].rename("employees")

        # 5) Company age
        if "founded_year" in companies_df.columns:
            feats["company_age"] = (
                datetime.now().year - companies_df["founded_year"]
            ).rename("company_age")

        # 6) Growth rate
        if "employees" in companies_df.columns:
            feats["growth_rate"] = self.calculate_growth_rate(companies_df)

        # 7) Urban/rural match
        if "location_type" in clubs_df.columns and "location_type" in companies_df.columns:
            feats["urban_rural_match"] = self.urban_rural_compatibility(
                clubs_df["location_type"], companies_df["location_type"]
            )

        return pd.DataFrame(feats)


================================================================================
FIL: sponsor_match/models/models.py
================================================================================

#!/usr/bin/env python3
"""
sponsor_match/models/models.py
-------------------------------
Ensemble of ML models for sponsorship probability prediction.
"""

import logging
from typing import Any, Dict, Union

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from sklearn.neural_network import MLPRegressor
import lightgbm as lgb

logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)

class SponsorshipPredictorEnsemble:
    """
    Holds a collection of models and provides a unified interface
    for training and predicting sponsor-match probabilities.
    """

    def __init__(self) -> None:
        """
        Initialize the ensemble with default hyperparameters.
        """
        self.models: Dict[str, Any] = {
            "rf": RandomForestRegressor(n_estimators=100),
            "gbm": GradientBoostingClassifier(),
            "lgbm": lgb.LGBMRegressor(),
            "nn": MLPRegressor(hidden_layer_sizes=(100, 50))
        }
        logger.info("Initialized SponsorshipPredictorEnsemble with models: %s",
                    list(self.models.keys()))

    def train(
        self,
        X_train: Union[pd.DataFrame, np.ndarray],
        y_train: Union[pd.Series, np.ndarray]
    ) -> None:
        """
        Fit each model in the ensemble on the training data.

        Parameters
        ----------
        X_train : DataFrame or ndarray
            Feature matrix.
        y_train : Series or ndarray
            Binary labels (1 = sponsored before, 0 = not).
        """
        for name, model in self.models.items():
            logger.info("Training model '%s'", name)
            model.fit(X_train, y_train)
        logger.info("All models trained successfully")

    def predict_proba(
        self,
        X: Union[pd.DataFrame, np.ndarray]
    ) -> np.ndarray:
        """
        Return the average predicted probability of sponsorship
        across all models in the ensemble.

        Parameters
        ----------
        X : DataFrame or ndarray
            Feature matrix.

        Returns
        -------
        ndarray
            Array of probabilities, one per row in X.
        """
        prob_list = []
        for name, model in self.models.items():
            if hasattr(model, "predict_proba"):
                probs = model.predict_proba(X)[:, 1]
                logger.debug("Model '%s' provided predict_proba output", name)
            else:
                # fallback: normalize regression output into [0,1]
                raw = model.predict(X)
                min_, max_ = raw.min(), raw.max()
                if max_ - min_ > 1e-8:
                    probs = (raw - min_) / (max_ - min_)
                else:
                    probs = np.zeros_like(raw)
                logger.debug("Model '%s' provided normalized regression output", name)
            prob_list.append(probs)

        # Ensemble by averaging
        ensemble_probs = np.mean(prob_list, axis=0)
        logger.info("Ensembled probabilities computed (shape=%s)", ensemble_probs.shape)
        return ensemble_probs


================================================================================
FIL: sponsor_match/data/__init__.py
================================================================================



================================================================================
FIL: sponsor_match/data/geocode_with_municipality.py
================================================================================

#!/usr/bin/env python3
"""
sponsor_match/data/geocode_with_municipality.py

Comprehensive geocoding system using Göteborg municipality data.
Handles associations and companies with various address formats.
"""

import logging
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple, Dict, List

import numpy as np
import pandas as pd
from fuzzywuzzy import fuzz, process
from geopy.extra.rate_limiter import RateLimiter
from geopy.geocoders import Nominatim

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class GeocodingResult:
    """Store geocoding results with confidence metadata."""
    lat: float
    lon: float
    district: str
    confidence: str  # 'exact', 'fuzzy', 'postcode', 'box', 'external'
    match_details: str
    original_address: str


class MunicipalityGeocoder:
    """
    Intelligent geocoder using Göteborg municipality data as primary source.
    Falls back to external services only when necessary.
    """

    def __init__(self, municipality_csv_path: str):
        """Initialize with municipality data."""
        logger.info("Loading municipality data...")
        # Try reading with UTF-8 encoding first, fall back to Latin-1 if needed
        try:
            self.muni_df = pd.read_csv(municipality_csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            logger.warning("UTF-8 decoding failed, trying Latin-1...")
            self.muni_df = pd.read_csv(municipality_csv_path, encoding='latin-1')

        # Fix encoding issues if they exist
        self._fix_encoding_issues()

        # Create various indices for efficient lookups
        self._prepare_indices()

        # Initialize external geocoder as fallback
        geolocator = Nominatim(user_agent="sponsor_match_geocoder")
        self.external_geocoder = RateLimiter(geolocator.geocode, min_delay_seconds=1)

        # Cache for Box addresses (will be populated on first use)
        self.box_address_cache = {}

        # Settings for controlling external geocoding
        self.use_external_geocoding = False  # Disable by default to avoid timeouts

    def _fix_encoding_issues(self):
        """Fix common UTF-8/Latin-1 encoding issues in Swedish text."""
        # Common misencoded Swedish character patterns
        encoding_fixes = {
            'ã¥': 'å', 'Ã¥': 'Å',
            'ã¤': 'ä', 'Ã¤': 'Ä',
            'ã¶': 'ö', 'Ã¶': 'Ö',
            'ã©': 'é', 'Ã©': 'É',
            'ã': 'à', 'Ã': 'À'
        }

        # Apply fixes to string columns
        for col in self.muni_df.columns:
            if self.muni_df[col].dtype == 'object':  # String columns
                for wrong, right in encoding_fixes.items():
                    self.muni_df[col] = self.muni_df[col].str.replace(wrong, right, regex=False)

        logger.info("Applied encoding fixes to municipality data")

    def _prepare_indices(self):
        """Prepare various indices for fast lookups."""
        # Clean and standardize street names
        self.muni_df['street_clean'] = (
            self.muni_df['STREET']
            .str.lower()
            .str.strip()
            .fillna('')
        )

        # Create full address for matching
        self.muni_df['full_address'] = (
                self.muni_df['street_clean'] + ' ' +
                self.muni_df['NUMBER'].fillna('').astype(str)
        ).str.strip()

        # Group by street for street-level lookups
        self.street_groups = self.muni_df.groupby('street_clean')

        # Create postcode index
        self.postcode_groups = self.muni_df.groupby('POSTCODE')

        # Get unique streets for fuzzy matching
        self.unique_streets = self.muni_df['street_clean'].unique()

        logger.info(f"Indexed {len(self.muni_df)} addresses across {len(self.unique_streets)} streets")

    def parse_address(self, raw_address: str) -> Dict[str, str]:
        """
        Parse various address formats into components.
        Handles contact names, Box addresses, and standard formats.
        """
        # Handle None or empty addresses
        if not raw_address or pd.isna(raw_address):
            return {'street': '', 'number': '', 'postcode': '', 'city': ''}

        address = str(raw_address).strip()

        # Check if it's a Box address
        box_match = re.search(r'Box\s*(\d+)', address, re.IGNORECASE)
        if box_match:
            return {
                'street': 'BOX',
                'number': box_match.group(1),
                'postcode': self._extract_postcode(address),
                'city': self._extract_city(address),
                'is_box': True
            }

        # Split by comma to separate components
        parts = [p.strip() for p in address.split(',')]

        # For associations, the middle part is often a contact name - skip it
        # Pattern: Street, ContactName, PostCode, City
        if len(parts) >= 4:
            street_part = parts[0]
            # Skip parts[1] as it's likely a contact name
            # Reconstruct address without contact name
            address = f"{parts[0]}, {parts[2]}, {parts[3]}"
        elif len(parts) >= 1:
            street_part = parts[0]
        else:
            street_part = ''

        # Extract postcode (5 digits, possibly with space)
        postcode = self._extract_postcode(address)

        # Extract house number from street part
        street, number = self._split_street_number(street_part)

        # Last part after postcode is usually city
        city = self._extract_city(address)

        return {
            'street': street,
            'number': number,
            'postcode': postcode,
            'city': city,
            'is_box': False
        }

    def _extract_postcode(self, address: str) -> str:
        """Extract Swedish postcode from address."""
        # Match 5 digits with optional space (e.g., "412 76" or "41276")
        match = re.search(r'(\d{3}\s?\d{2})', address)
        if match:
            # Normalize to no-space format
            return match.group(1).replace(' ', '')
        return ''

    def _extract_city(self, address: str) -> str:
        """Extract city name from address."""
        # Common districts/cities in Göteborg area
        cities = ['Göteborg', 'GÖTEBORG', 'Mölndal', 'MÖLNDAL', 'Partille', 'PARTILLE',
                  'Angered', 'ANGERED', 'Västra Frölunda', 'VÄSTRA FRÖLUNDA',
                  'Hisings Backa', 'HISINGS BACKA', 'Torslanda', 'TORSLANDA']

        for city in cities:
            if city in address:
                return city.title()
        return 'Göteborg'  # Default

    def _split_street_number(self, street_part: str) -> Tuple[str, str]:
        """Split street name and house number."""
        # Try to find number at the end
        match = re.match(r'^(.+?)\s+(\d+\s*[A-Za-z]?)$', street_part)
        if match:
            return match.group(1).strip(), match.group(2).strip()

        # No number found
        return street_part.strip(), ''

    def geocode_address(self, raw_address: str) -> Optional[GeocodingResult]:
        """
        Main geocoding method with multiple fallback strategies.
        """
        # Parse the address
        parsed = self.parse_address(raw_address)

        # Handle Box addresses specially
        if parsed.get('is_box'):
            return self._geocode_box_address(parsed, raw_address)

        # Try exact match first
        result = self._try_exact_match(parsed, raw_address)
        if result:
            return result

        # Try fuzzy street matching
        result = self._try_fuzzy_match(parsed, raw_address)
        if result:
            return result

        # Try postcode-based geocoding
        result = self._try_postcode_match(parsed, raw_address)
        if result:
            return result

        # Last resort: external geocoding
        return self._try_external_geocoding(raw_address)

    def _try_exact_match(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """Try exact street + number match."""
        street_clean = parsed['street'].lower().strip()
        number = parsed['number']

        if not street_clean:
            return None

        # Get all addresses on this street
        if street_clean in self.street_groups.groups:
            street_data = self.street_groups.get_group(street_clean)

            if number:
                # Try exact number match
                exact_match = street_data[street_data['NUMBER'].astype(str) == str(number)]
                if not exact_match.empty:
                    row = exact_match.iloc[0]
                    return GeocodingResult(
                        lat=row['LAT'],
                        lon=row['LON'],
                        district=row['DISTRICT'],
                        confidence='exact',
                        match_details=f"Exact match: {row['STREET']} {row['NUMBER']}",
                        original_address=original
                    )

            # No number or no exact match - use street midpoint
            return self._get_street_midpoint(street_data, parsed, original)

        return None

    def _try_fuzzy_match(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """Try fuzzy matching for street names."""
        street_clean = parsed['street'].lower().strip()
        if not street_clean or len(street_clean) < 3:
            return None

        # Find best matching street
        best_match = process.extractOne(
            street_clean,
            self.unique_streets,
            scorer=fuzz.ratio,
            score_cutoff=70  # Lower threshold to handle encoding issues
        )

        if best_match:
            matched_street, score = best_match[0], best_match[1]
            street_data = self.street_groups.get_group(matched_street)

            logger.info(f"Fuzzy matched '{street_clean}' to '{matched_street}' (score: {score})")

            # Try to find specific number or use midpoint
            if parsed['number']:
                number_match = street_data[street_data['NUMBER'].astype(str) == str(parsed['number'])]
                if not number_match.empty:
                    row = number_match.iloc[0]
                    return GeocodingResult(
                        lat=row['LAT'],
                        lon=row['LON'],
                        district=row['DISTRICT'],
                        confidence='fuzzy',
                        match_details=f"Fuzzy match: {row['STREET']} {row['NUMBER']} (score: {score})",
                        original_address=original
                    )

            return self._get_street_midpoint(street_data, parsed, original, confidence='fuzzy')

        return None

    def _get_street_midpoint(self, street_data: pd.DataFrame, parsed: Dict,
                             original: str, confidence: str = 'exact') -> GeocodingResult:
        """Calculate midpoint of all addresses on a street."""
        # Use the centroid of all points on the street
        lat_mean = street_data['LAT'].mean()
        lon_mean = street_data['LON'].mean()

        # Use most common district
        district = street_data['DISTRICT'].mode().iloc[0] if not street_data['DISTRICT'].mode().empty else ''

        return GeocodingResult(
            lat=lat_mean,
            lon=lon_mean,
            district=district,
            confidence=confidence,
            match_details=f"Street midpoint: {street_data.iloc[0]['STREET']} ({len(street_data)} addresses)",
            original_address=original
        )

    def _try_postcode_match(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """Use postcode centroid as fallback."""
        postcode = parsed['postcode']
        if not postcode:
            return None

        if postcode in self.postcode_groups.groups:
            postcode_data = self.postcode_groups.get_group(postcode)

            # Use centroid of all addresses in this postcode
            lat_mean = postcode_data['LAT'].mean()
            lon_mean = postcode_data['LON'].mean()
            district = postcode_data['DISTRICT'].mode().iloc[0] if not postcode_data['DISTRICT'].mode().empty else ''

            return GeocodingResult(
                lat=lat_mean,
                lon=lon_mean,
                district=district,
                confidence='postcode',
                match_details=f"Postcode centroid: {postcode} ({len(postcode_data)} addresses)",
                original_address=original
            )

        return None

    def _geocode_box_address(self, parsed: Dict, original: str) -> Optional[GeocodingResult]:
        """
        Handle Box addresses. In Sweden, these are typically at post offices.
        We'll need external geocoding for these.
        """
        box_number = parsed['number']
        postcode = parsed['postcode']
        city = parsed['city']

        # Create a searchable address
        search_address = f"Box {box_number}, {postcode} {city}, Sverige"

        # Check cache first
        if search_address in self.box_address_cache:
            cached = self.box_address_cache[search_address]
            return GeocodingResult(
                lat=cached['lat'],
                lon=cached['lon'],
                district=cached.get('district', ''),
                confidence='box',
                match_details=f"Box address (cached): {search_address}",
                original_address=original
            )

        # Try external geocoding
        result = self._try_external_geocoding(search_address)
        if result:
            result.confidence = 'box'
            result.match_details = f"Box address (geocoded): {search_address}"
            # Cache for future use
            self.box_address_cache[search_address] = {
                'lat': result.lat,
                'lon': result.lon,
                'district': result.district
            }

        return result

    def _try_external_geocoding(self, address: str) -> Optional[GeocodingResult]:
        """Use external geocoding service as last resort."""
        # Check if external geocoding is enabled
        if not self.use_external_geocoding:
            logger.debug(f"External geocoding disabled, skipping: {address}")
            return None

        try:
            # Add country for better results
            if "Sverige" not in address and "Sweden" not in address:
                address += ", Sverige"

            location = self.external_geocoder(address)
            if location:
                # Try to determine district from coordinates
                district = self._find_district_for_coords(location.latitude, location.longitude)

                return GeocodingResult(
                    lat=location.latitude,
                    lon=location.longitude,
                    district=district,
                    confidence='external',
                    match_details=f"External geocoding: {location.address}",
                    original_address=address
                )
        except Exception as e:
            logger.error(f"External geocoding failed for '{address}': {e}")

        return None

    def _find_district_for_coords(self, lat: float, lon: float, radius_km: float = 1.0) -> str:
        """Find district for given coordinates by finding nearest address."""
        # Calculate distances to all addresses (simplified, not haversine)
        self.muni_df['dist'] = np.sqrt(
            (self.muni_df['LAT'] - lat) ** 2 +
            (self.muni_df['LON'] - lon) ** 2
        )

        # Find nearest address
        nearest = self.muni_df.nsmallest(1, 'dist')
        if not nearest.empty:
            return nearest.iloc[0]['DISTRICT']

        return ''

    def geocode_batch(self, addresses: List[str], desc: str = "Geocoding") -> List[Optional[GeocodingResult]]:
        """Geocode a batch of addresses with progress tracking."""
        results = []

        logger.info(f"Starting batch geocoding of {len(addresses)} addresses...")

        for i, address in enumerate(addresses):
            if i % 100 == 0:
                logger.info(f"{desc}: {i}/{len(addresses)} ({i / len(addresses) * 100:.1f}%)")

            result = self.geocode_address(address)
            results.append(result)

            # Small delay to prevent overwhelming external service
            if result and result.confidence == 'external':
                time.sleep(0.1)

        # Summary statistics
        confidences = [r.confidence for r in results if r]
        for conf in ['exact', 'fuzzy', 'postcode', 'box', 'external']:
            count = confidences.count(conf)
            if count > 0:
                logger.info(f"  {conf}: {count} ({count / len(addresses) * 100:.1f}%)")

        failed = len([r for r in results if r is None])
        if failed > 0:
            logger.warning(f"  Failed: {failed} ({failed / len(addresses) * 100:.1f}%)")

        return results


def geocode_associations(associations_csv: str, municipality_csv: str, output_csv: str):
    """Geocode all associations and save results."""
    logger.info("Loading associations...")
    assoc_df = pd.read_csv(associations_csv, encoding='utf-8')

    # Initialize geocoder
    geocoder = MunicipalityGeocoder(municipality_csv)

    # Extract addresses (handling the specific format)
    addresses = []
    for _, row in assoc_df.iterrows():
        # Combine address components, skipping Co Adress
        address_parts = []
        if pd.notna(row.get('Adress')):
            address_parts.append(str(row['Adress']))
        if pd.notna(row.get('Post Nr')):
            address_parts.append(str(row['Post Nr']))
        if pd.notna(row.get('Postort')):
            address_parts.append(str(row['Postort']))

        address = ', '.join(address_parts)
        addresses.append(address)

    # Geocode all addresses
    results = geocoder.geocode_batch(addresses, desc="Associations")

    # Add results to dataframe
    assoc_df['lat'] = [r.lat if r else None for r in results]
    assoc_df['lon'] = [r.lon if r else None for r in results]
    assoc_df['district'] = [r.district if r else None for r in results]
    assoc_df['geocoding_confidence'] = [r.confidence if r else 'failed' for r in results]
    assoc_df['geocoding_details'] = [r.match_details if r else 'Geocoding failed' for r in results]

    # Save results
    assoc_df.to_csv(output_csv, index=False, encoding='utf-8')
    logger.info(f"Saved geocoded associations to {output_csv}")

    # Report success rate
    success_rate = (assoc_df['lat'].notna().sum() / len(assoc_df)) * 100
    logger.info(f"Successfully geocoded {success_rate:.1f}% of associations")


def geocode_companies(companies_csv: str, municipality_csv: str, output_csv: str):
    """Geocode all companies and save results."""
    logger.info("Loading companies...")
    comp_df = pd.read_csv(companies_csv, encoding='utf-8')

    # Initialize geocoder
    geocoder = MunicipalityGeocoder(municipality_csv)

    # Extract addresses
    addresses = comp_df['registered_address'].tolist()

    # Geocode all addresses
    results = geocoder.geocode_batch(addresses, desc="Companies")

    # Add results to dataframe
    comp_df['lat'] = [r.lat if r else None for r in results]
    comp_df['lon'] = [r.lon if r else None for r in results]
    comp_df['geocoding_confidence'] = [r.confidence if r else 'failed' for r in results]
    comp_df['geocoding_details'] = [r.match_details if r else 'Geocoding failed' for r in results]

    # Verify district matches
    comp_df['district_verified'] = [
        r.district == row['district'] if r and r.district else False
        for r, (_, row) in zip(results, comp_df.iterrows())
    ]

    # Save results
    comp_df.to_csv(output_csv, index=False, encoding='utf-8')
    logger.info(f"Saved geocoded companies to {output_csv}")

    # Report success rate
    success_rate = (comp_df['lat'].notna().sum() / len(comp_df)) * 100
    logger.info(f"Successfully geocoded {success_rate:.1f}% of companies")


def main():
    """Main function to geocode both associations and companies."""
    # Set up paths
    project_root = Path(__file__).parent.parent.parent
    data_dir = project_root / "data"

    municipality_csv = data_dir / "municipality_of_goteborg.csv"
    associations_csv = data_dir / "gothenburg_associations.csv"
    companies_csv = data_dir / "gothenburg_companies_addresses.csv"

    # Output files
    associations_output = data_dir / "associations_geocoded.csv"
    companies_output = data_dir / "companies_geocoded.csv"

    # Geocode associations
    logger.info("=" * 50)
    logger.info("GEOCODING ASSOCIATIONS")
    logger.info("=" * 50)
    geocode_associations(str(associations_csv), str(municipality_csv), str(associations_output))

    # Geocode companies
    logger.info("\n" + "=" * 50)
    logger.info("GEOCODING COMPANIES")
    logger.info("=" * 50)
    geocode_companies(str(companies_csv), str(municipality_csv), str(companies_output))

    logger.info("\nGeocoding complete!")


if __name__ == "__main__":
    main()


================================================================================
FIL: sponsor_match/data/ingest_associations_fixed.py
================================================================================

#!/usr/bin/env python3
"""
Ingest Gothenburg associations with geocoding and proper data cleaning.
"""

import logging
import time
from pathlib import Path

import pandas as pd
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter
from sqlalchemy import text

from sponsor_match.core.db import get_engine
from sponsor_match.models.entities import Base, Association

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def clean_associations_data(df):
    """Clean and prepare associations data."""
    # Create full address
    df['full_address'] = df['Adress'].fillna('') + ', ' + \
                         df['Post Nr'].fillna('').astype(str) + ' ' + \
                         df['Postort'].fillna('')

    # Clean up whitespace
    df['full_address'] = df['full_address'].str.strip().str.replace('  ', ' ')

    # Add ", Sverige" for better geocoding
    df['full_address'] = df['full_address'] + ', Sverige'

    return df


def geocode_addresses(df, limit=50):
    """Geocode addresses with rate limiting."""
    geolocator = Nominatim(user_agent="sponsormatch_geocoder")
    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)

    geocoded = []

    for idx, row in df.iterrows():
        if idx >= limit:  # Limit for testing
            break

        try:
            location = geocode(row['full_address'])
            if location:
                geocoded.append({
                    'name': row['Namn'],
                    'address': row['full_address'],
                    'lat': location.latitude,
                    'lon': location.longitude
                })
                logger.info(f"Geocoded: {row['Namn']} -> {location.latitude}, {location.longitude}")
            else:
                # Try with just city name
                location = geocode(f"{row['Postort']}, Sverige")
                if location:
                    geocoded.append({
                        'name': row['Namn'],
                        'address': row['full_address'],
                        'lat': location.latitude,
                        'lon': location.longitude
                    })
                    logger.info(f"Geocoded (city): {row['Namn']} -> {location.latitude}, {location.longitude}")
                else:
                    logger.warning(f"Could not geocode: {row['Namn']}")

        except Exception as e:
            logger.error(f"Geocoding error for {row['Namn']}: {e}")
            time.sleep(2)  # Extra delay on error

    return pd.DataFrame(geocoded)


def assign_size_buckets(df):
    """Assign size buckets based on name patterns and random distribution."""
    import random

    # Keywords that suggest size
    large_keywords = ['IFK', 'GAIS', 'BK Häcken', 'Örgryte']
    small_keywords = ['FF', 'Futsal', 'Ungdom']

    def get_size_bucket(name):
        name_upper = name.upper()

        # Check for large club indicators
        for keyword in large_keywords:
            if keyword.upper() in name_upper:
                return 'large'

        # Check for small club indicators
        for keyword in small_keywords:
            if keyword.upper() in name_upper:
                return 'small'

        # Random distribution for others
        rand = random.random()
        if rand < 0.3:
            return 'small'
        elif rand < 0.7:
            return 'medium'
        else:
            return 'large'

    df['size_bucket'] = df['name'].apply(get_size_bucket)

    # Assign member counts based on size
    def get_member_count(size):
        if size == 'small':
            return random.randint(50, 150)
        elif size == 'medium':
            return random.randint(151, 500)
        else:
            return random.randint(501, 1500)

    df['member_count'] = df['size_bucket'].apply(get_member_count)
    df['founded_year'] = [random.randint(1900, 2020) for _ in range(len(df))]

    return df


def main():
    """Main ingestion function."""
    # Read the CSV
    csv_path = Path("data/gothenburg_associations.csv")

    try:
        # Try different encodings
        for encoding in ['utf-8', 'latin-1', 'iso-8859-1']:
            try:
                df = pd.read_csv(csv_path, encoding=encoding)
                logger.info(f"Successfully read CSV with {encoding} encoding")
                break
            except UnicodeDecodeError:
                continue
    except Exception as e:
        logger.error(f"Could not read CSV: {e}")
        return

    # Filter out empty rows
    df = df[df['Namn'].notna()].copy()
    logger.info(f"Found {len(df)} associations")

    # Clean data
    df = clean_associations_data(df)

    # Geocode addresses (limited for testing)
    logger.info("Starting geocoding...")
    geocoded_df = geocode_addresses(df, limit=30)  # Limit to 30 for testing

    if geocoded_df.empty:
        logger.error("No addresses geocoded")
        return

    # Add size buckets and other fields
    geocoded_df = assign_size_buckets(geocoded_df)

    # Prepare for database
    geocoded_df['id'] = range(1, len(geocoded_df) + 1)

    # Insert into database
    engine = get_engine()

    # Create tables if needed
    Base.metadata.create_all(bind=engine)

    # Clear existing data
    with engine.begin() as conn:
        conn.execute(text("DELETE FROM associations"))

        # Insert new data
        geocoded_df.to_sql(
            'associations',
            conn,
            if_exists='append',
            index=False,
            method='multi'
        )

    logger.info(f"Successfully loaded {len(geocoded_df)} associations")

    # Add some sample data for major clubs if not geocoded
    add_sample_major_clubs(engine)


def add_sample_major_clubs(engine):
    """Add well-known Gothenburg clubs with accurate coordinates."""
    major_clubs = [
        {
            'name': 'IFK Göteborg',
            'address': 'Kamratgårdsvägen 50, 416 55 Göteborg, Sverige',
            'lat': 57.706547,
            'lon': 11.980125,
            'size_bucket': 'large',
            'member_count': 1500,
            'founded_year': 1904
        },
        {
            'name': 'GAIS',
            'address': 'Gamla Boråsvägen 75, 412 76 Göteborg, Sverige',
            'lat': 57.687932,
            'lon': 11.989746,
            'size_bucket': 'large',
            'member_count': 1200,
            'founded_year': 1894
        },
        {
            'name': 'BK Häcken',
            'address': 'Entreprenadvägen 6, 417 05 Göteborg, Sverige',
            'lat': 57.705891,
            'lon': 11.936847,
            'size_bucket': 'large',
            'member_count': 1000,
            'founded_year': 1940
        }
    ]

    # Check if these already exist
    with engine.begin() as conn:
        for club in major_clubs:
            exists = conn.execute(
                text("SELECT 1 FROM associations WHERE name = :name"),
                {"name": club['name']}
            ).first()

            if not exists:
                conn.execute(
                    text("""
                        INSERT INTO associations 
                        (name, address, lat, lon, size_bucket, member_count, founded_year)
                        VALUES (:name, :address, :lat, :lon, :size_bucket, :member_count, :founded_year)
                    """),
                    club
                )
                logger.info(f"Added major club: {club['name']}")


if __name__ == "__main__":
    main()


================================================================================
FIL: sponsor_match/data/ingest_companies.py
================================================================================

#!/usr/bin/env python3
"""
sponsor_match/data/ingest_companies.py
-----------------------------------------
Read data/bolag_1_500_with_coords.csv (already geocoded) and load into companies table.
"""

import sys
import logging
from pathlib import Path
import pandas as pd
from sqlalchemy import text
from sponsor_match.core.db import get_engine

# Configure logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def main() -> None:
    # Use the already geocoded file
    project_root = Path(__file__).resolve().parents[2]
    csv_path = project_root / "data" / "bolag_1_500_with_coords.csv"

    # Read the geocoded file
    try:
        df = pd.read_csv(csv_path, encoding="utf-8")
        logger.info("Loaded %d rows from %s", len(df), csv_path)
    except FileNotFoundError:
        logger.error("CSV not found at %s", csv_path)
        sys.exit(1)
    except Exception as e:
        logger.exception("Unexpected read error: %s", e)
        sys.exit(1)

    # Rename columns to match database schema
    df = df.rename(columns={
        "Företagsnamn": "name",
        "Postadress": "address",
        "Omsättning (tkr)": "revenue_ksek",
        "Anställda": "employees",
        "År": "year",
    })

    # Calculate size bucket based on revenue
    def calculate_size_bucket(revenue_ksek):
        if pd.isna(revenue_ksek):
            return "medium"
        revenue_sek = revenue_ksek * 1000
        if revenue_sek < 5_000_000:
            return "small"
        elif revenue_sek < 50_000_000:
            return "medium"
        else:
            return "large"

    df["size_bucket"] = df["revenue_ksek"].apply(calculate_size_bucket)

    # Add default industry and orgnr to match database schema
    df["industry"] = "Other"
    df["orgnr"] = None  # No organization numbers in our data

    # Select final columns matching database schema
    final_columns = ["orgnr", "name", "revenue_ksek", "employees", "year", "size_bucket", "industry", "lat", "lon"]
    df = df[final_columns]

    # Drop rows with missing coordinates
    before_count = len(df)
    df = df.dropna(subset=["lat", "lon"])
    after_count = len(df)
    if before_count > after_count:
        logger.warning("Dropped %d rows with missing coordinates", before_count - after_count)

    # Write to database
    engine = get_engine()

    try:
        with engine.begin() as conn:
            # Clear existing data and insert new
            conn.execute(text("DELETE FROM companies"))
            df.to_sql("companies", conn, if_exists="append", index=False)

            new_count = conn.execute(text("SELECT COUNT(*) FROM companies")).scalar() or 0
            logger.info("✅ Companies ingestion complete. Total rows: %d", new_count)

    except Exception as e:
        logger.exception("DB error during ingest: %s", e)
        sys.exit(1)


if __name__ == "__main__":
    main()

================================================================================
FIL: utils/__init__.py
================================================================================



================================================================================
FIL: utils/check_db.py
================================================================================

#!/usr/bin/env python3
"""
utils/check_db.py

Utility to verify database connectivity and inspect tables.
"""

import logging
import sys
from dotenv import load_dotenv
from sqlalchemy import inspect
from sponsor_match.core.db import get_engine

def main():
    # Load environment variables (e.g. DATABASE_URL)
    load_dotenv()

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s"
    )

    # Create/get the SQLAlchemy engine
    try:
        engine = get_engine()
    except RuntimeError as e:
        logging.error(f"Could not create database engine: {e}")
        sys.exit(1)

    # Inspect the database
    inspector = inspect(engine)
    tables = inspector.get_table_names()
    if not tables:
        logging.warning("No tables found in the database.")
        return

    logging.info(f"Found tables: {tables}")

    # For each table, count rows
    for table in tables:
        try:
            with engine.connect() as conn:
                result = conn.execute(f"SELECT COUNT(*) FROM {table}")
                count = result.scalar()
            logging.info(f"Table '{table}' has {count} records.")
        except Exception as e:
            logging.error(f"Error querying table '{table}': {e}")

if __name__ == "__main__":
    main()


================================================================================
FIL: utils/generate_sample_data.py
================================================================================

#!/usr/bin/env python3
"""Generate realistic sample data for SponsorMatch AI."""

import pandas as pd
import random
from faker import Faker
import numpy as np
from pathlib import Path

# Set seeds for reproducibility
random.seed(42)
np.random.seed(42)
fake = Faker()
Faker.seed(42)


def generate_sample_companies(n=50):
    """Generate realistic Swedish companies."""
    companies = []

    # Swedish company types and their characteristics
    industries = {
        'Technology': {'revenue_range': (10000, 500000), 'employee_range': (10, 200)},
        'Manufacturing': {'revenue_range': (50000, 1000000), 'employee_range': (50, 500)},
        'Finance': {'revenue_range': (25000, 750000), 'employee_range': (20, 300)},
        'Healthcare': {'revenue_range': (15000, 400000), 'employee_range': (15, 150)},
        'Retail': {'revenue_range': (20000, 300000), 'employee_range': (25, 100)},
    }

    # Swedish cities with coordinates
    cities = [
        ('Göteborg', 57.7089, 11.9746),
        ('Stockholm', 59.3293, 18.0686),
        ('Malmö', 55.6050, 13.0038),
        ('Uppsala', 59.8586, 17.6389),
        ('Linköping', 58.4108, 15.6214)
    ]

    for i in range(n):
        industry = random.choice(list(industries.keys()))
        city, base_lat, base_lon = random.choice(cities)

        # Add some randomness to coordinates
        lat = base_lat + random.uniform(-0.1, 0.1)
        lon = base_lon + random.uniform(-0.1, 0.1)

        revenue = random.randint(*industries[industry]['revenue_range'])
        employees = random.randint(*industries[industry]['employee_range'])

        # Determine size bucket
        if revenue < 50000:
            size_bucket = 'small'
        elif revenue < 250000:
            size_bucket = 'medium'
        else:
            size_bucket = 'large'

        company = {
            'id': i + 1,
            'orgnr': f"{random.randint(100000, 999999)}-{random.randint(1000, 9999)}",
            'name': f"{fake.company()} {random.choice(['AB', 'Ltd', 'Group'])}",
            'revenue_ksek': revenue,
            'employees': employees,
            'year': random.randint(2020, 2024),
            'size_bucket': size_bucket,
            'industry': industry,
            'lat': round(lat, 6),
            'lon': round(lon, 6)
        }
        companies.append(company)

    return pd.DataFrame(companies)


def generate_sample_associations(n=30):
    """Generate realistic Swedish sports associations."""
    associations = []

    sports = ['Fotboll', 'Ishockey', 'Bandy', 'Handboll', 'Basket', 'Innebandy']
    cities = [
        ('Göteborg', 57.7089, 11.9746),
        ('Mölndal', 57.6554, 12.0134),
        ('Partille', 57.7394, 12.1065),
        ('Lerum', 57.7706, 12.2694),
        ('Kungälv', 57.8700, 11.9800)
    ]

    for i in range(n):
        sport = random.choice(sports)
        city, base_lat, base_lon = random.choice(cities)

        lat = base_lat + random.uniform(-0.05, 0.05)
        lon = base_lon + random.uniform(-0.05, 0.05)

        members = random.randint(50, 800)

        # Size bucket based on members
        if members < 150:
            size_bucket = 'small'
        elif members < 400:
            size_bucket = 'medium'
        else:
            size_bucket = 'large'

        association = {
            'id': i + 1,
            'name': f"{city} {sport}klub",
            'member_count': members,
            'address': f"{fake.street_address()}, {city}",
            'lat': round(lat, 6),
            'lon': round(lon, 6),
            'size_bucket': size_bucket,
            'founded_year': random.randint(1950, 2020)
        }
        associations.append(association)

    return pd.DataFrame(associations)


def main():
    """Generate and save sample data."""
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data"
    data_dir.mkdir(exist_ok=True)

    # Generate data
    print("Generating sample companies...")
    companies = generate_sample_companies(50)
    companies.to_csv(data_dir / "sample_companies.csv", index=False)
    print(f"Generated {len(companies)} companies")

    print("Generating sample associations...")
    associations = generate_sample_associations(30)
    associations.to_csv(data_dir / "sample_associations.csv", index=False)
    print(f"Generated {len(associations)} associations")

    print(f"Data saved to {data_dir}")


if __name__ == "__main__":
    main()


================================================================================
FIL: utils/list_project_files.py
================================================================================

#!/usr/bin/env python3
"""
utils/list_project_files.py
----------------------
Recursively scans the entire project root (one level up from this script), skips
directories .venv and .venv312 as well as __pycache__ and .git, and writes
out both filenames and entire contents for file types:
.py, .csv, .json, .toml, .md and .yml

Now creates four separate dump files to split the content evenly.

Usage:
    cd /home/user/SponsorMatchAI
    python utils/list_project_files.py [--output-prefix CUSTOM_PREFIX]

Output:
    utils_output/project_dump_part1.txt through utils_output/project_dump_part4.txt (default)
"""

import argparse
import logging
import os
from pathlib import Path
from typing import List, Set

# Configure logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

EXCLUDE_DIRS = {'.venv', '.venv312', '__pycache__', '.git'}
EXCLUDE_FILES = {
    'data/bolag_1_500_sorted_with_year.csv',
    'data/associations_goteborg.csv',
    'data/associations_goteborg_with_coords.csv',
    'data/gothenburg_companies_addresses.csv',
    'data/associations_geocoded.csv',
    'data/companies_geocoded.csv',
    'data/gothenburg_associations.csv',
    'data/municipality_of_goteborg.csv'
}
EXCLUDE_FILENAMES = {'bolag_1_500_with_coords.csv'}


def collect_files(root: Path, exts: List[str], exclude_files: Set[str], exclude_filenames: Set[str]) -> List[Path]:
    """
    Go through root and all subdirectories, collect files with suffixes in exts,
    skip EXCLUDE_DIRS, exclude_files, and exclude_filenames, and return a list of file paths.
    """
    collected_files = []
    for dirpath, dirnames, filenames in os.walk(root):
        # Exclude unwanted directories
        dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS]
        for fn in sorted(filenames):
            # Skip files with excluded filenames
            if fn in exclude_filenames:
                logger.info(f"Excluding file by name: {fn} in {dirpath}")
                continue

            if any(fn.lower().endswith(ext) for ext in exts):
                file_path = Path(dirpath) / fn
                rel_path = str(file_path.relative_to(root))

                # Skip excluded files by path
                if rel_path in exclude_files:
                    logger.info(f"Excluding file by path: {rel_path}")
                    continue

                collected_files.append(file_path)
    return collected_files


def write_files_to_dump(files: List[Path], root: Path, out_path: Path) -> None:
    """
    Write the given files to the output dump file.
    """
    with out_path.open("w", encoding="utf-8") as f:
        for file_path in files:
            f.write("=" * 80 + "\n")
            f.write(f"FIL: {file_path.relative_to(root)}\n")
            f.write("=" * 80 + "\n\n")
            try:
                content = file_path.read_text(encoding="utf-8")
                f.write(content)
            except Exception as e:
                f.write(f"<Could not read file: {e}>\n")
            f.write("\n\n")
    size_kb = out_path.stat().st_size / 1024
    logger.info("Created dump file %s (%.1f KB)", out_path, size_kb)


def split_into_four_parts(files: List[Path]) -> List[List[Path]]:
    """
    Split the list of files into four approximately equal parts.
    """
    file_count = len(files)
    quarter = file_count // 4

    # Handle case where file count doesn't divide evenly by 4
    # by distributing remainder to first parts
    remainder = file_count % 4

    # Calculate lengths for each part
    lengths = [quarter + (1 if i < remainder else 0) for i in range(4)]

    # Create the parts
    result = []
    start_idx = 0
    for length in lengths:
        end_idx = start_idx + length
        result.append(files[start_idx:end_idx])
        start_idx = end_idx

    return result


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Create four dump files of important project files with full content."
    )
    parser.add_argument(
        "--output-prefix", "-o",
        type=str,
        default=None,
        help="Output file prefix (default: utils_output/project_dump)"
    )
    args = parser.parse_args()

    # Since we're now in utils/ directory, need to get the project root (one level up)
    project_root = Path(__file__).parent.parent

    # Create utils_output directory if it doesn't exist
    outputs_dir = project_root / "utils_output"
    outputs_dir.mkdir(exist_ok=True)

    # Set default output prefix if not specified
    output_prefix = args.output_prefix if args.output_prefix else outputs_dir / "project_dump"

    extensions = [".py", ".csv", ".json", ".toml", ".md", ".yml"]
    logger.info("Scanning %s for %s files...", project_root, extensions)

    # Collect all relevant files, excluding specific files
    all_files = collect_files(project_root, extensions, EXCLUDE_FILES, EXCLUDE_FILENAMES)
    logger.info("Found %d files to include in dumps", len(all_files))

    # Split files into four groups
    file_parts = split_into_four_parts(all_files)

    # Write each group to a separate dump file
    for i, files_part in enumerate(file_parts, 1):
        output_path = Path(f"{output_prefix}_part{i}.txt")
        write_files_to_dump(files_part, project_root, output_path)
        logger.info("Part %d contains %d files", i, len(files_part))

    logger.info("Created four dump files with prefix: %s", output_prefix)


if __name__ == "__main__":
    main()


================================================================================
FIL: utils/setup_database.py
================================================================================

#!/usr/bin/env python3
"""
setup_database.py - Complete database setup for SponsorMatch AI
Ensures both associations and companies are properly loaded.
"""

import logging
from pathlib import Path

import pandas as pd
from sqlalchemy import text

from sponsor_match.core.db import get_engine
from sponsor_match.models.entities import Base

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def setup_database():
    """Set up the complete database with sample data."""
    engine = get_engine()

    # Create all tables
    Base.metadata.create_all(bind=engine)
    logger.info("Created database tables")

    # Load associations
    load_sample_associations(engine)

    # Load companies
    load_sample_companies(engine)

    # Verify data
    verify_data(engine)


def load_sample_associations(engine):
    """Load sample associations with realistic Gothenburg data."""
    associations = [
        # Large clubs
        {'name': 'IFK Göteborg', 'address': 'Kamratgårdsvägen 50, 416 55 Göteborg',
         'lat': 57.706547, 'lon': 11.980125, 'size_bucket': 'large', 'member_count': 1500, 'founded_year': 1904},
        {'name': 'GAIS', 'address': 'Gamla Boråsvägen 75, 412 76 Göteborg',
         'lat': 57.687932, 'lon': 11.989746, 'size_bucket': 'large', 'member_count': 1200, 'founded_year': 1894},
        {'name': 'BK Häcken', 'address': 'Entreprenadvägen 6, 417 05 Göteborg',
         'lat': 57.705891, 'lon': 11.936847, 'size_bucket': 'large', 'member_count': 1000, 'founded_year': 1940},
        {'name': 'Örgryte IS', 'address': 'Skånegatan 5, 412 51 Göteborg',
         'lat': 57.693734, 'lon': 11.996542, 'size_bucket': 'large', 'member_count': 900, 'founded_year': 1887},

        # Medium clubs
        {'name': 'Qviding FIF', 'address': 'Härlanda Park 6B, 416 52 Göteborg',
         'lat': 57.703456, 'lon': 12.015234, 'size_bucket': 'medium', 'member_count': 400, 'founded_year': 1987},
        {'name': 'Västra Frölunda IF', 'address': 'Klubbvägen 19, 421 47 Västra Frölunda',
         'lat': 57.652341, 'lon': 11.928765, 'size_bucket': 'medium', 'member_count': 350, 'founded_year': 1947},
        {'name': 'Kärra KIF', 'address': 'Burmans gata 3, 425 33 Hisings Kärra',
         'lat': 57.765432, 'lon': 11.945678, 'size_bucket': 'medium', 'member_count': 300, 'founded_year': 1970},
        {'name': 'Sävedalens IF', 'address': 'Hultvägen 2, 433 64 Sävedalen',
         'lat': 57.709876, 'lon': 12.057891, 'size_bucket': 'medium', 'member_count': 450, 'founded_year': 1948},

        # Small clubs
        {'name': 'Majorna BK', 'address': 'Karl Johansgatan 152, 414 51 Göteborg',
         'lat': 57.689012, 'lon': 11.914567, 'size_bucket': 'small', 'member_count': 120, 'founded_year': 1990},
        {'name': 'Lundby IF', 'address': 'Munkedalsgatan 10, 417 16 Göteborg',
         'lat': 57.719234, 'lon': 11.942345, 'size_bucket': 'small', 'member_count': 100, 'founded_year': 2005},
        {'name': 'Gamlestaden FF', 'address': 'Artillerigatan 33, 415 02 Göteborg',
         'lat': 57.725678, 'lon': 12.005432, 'size_bucket': 'small', 'member_count': 80, 'founded_year': 2010},
        {'name': 'Kortedala IK', 'address': 'Julaftonsgatan 58, 415 44 Göteborg',
         'lat': 57.737890, 'lon': 12.026789, 'size_bucket': 'small', 'member_count': 90, 'founded_year': 1969},
    ]

    df = pd.DataFrame(associations)

    with engine.begin() as conn:
        # Clear existing data
        conn.execute(text("DELETE FROM associations"))

        # Insert new data
        df.to_sql('associations', conn, if_exists='append', index=False)

    logger.info(f"Loaded {len(df)} associations")


def load_sample_companies(engine):
    """Load sample companies in Gothenburg area."""
    companies = []

    # Technology companies (10-char orgnr format)
    companies.extend([
        {'name': 'Volvo Tech AB', 'orgnr': '5560123456', 'revenue_ksek': 150000, 'employees': 200,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Technology', 'lat': 57.708765, 'lon': 11.965432},
        {'name': 'Ericsson Göteborg', 'orgnr': '5560234567', 'revenue_ksek': 200000, 'employees': 300,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Technology', 'lat': 57.695432, 'lon': 11.987654},
        {'name': 'IT Solutions AB', 'orgnr': '5560345678', 'revenue_ksek': 25000, 'employees': 50,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Technology', 'lat': 57.712345, 'lon': 11.998765},
    ])

    # Finance companies
    companies.extend([
        {'name': 'Nordea Göteborg', 'orgnr': '5560456789', 'revenue_ksek': 300000, 'employees': 150,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Finance', 'lat': 57.701234, 'lon': 11.975432},
        {'name': 'SEB Private Banking', 'orgnr': '5560567890', 'revenue_ksek': 180000, 'employees': 80,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Finance', 'lat': 57.698765, 'lon': 11.968901},
        {'name': 'Finanskonsult AB', 'orgnr': '5560678901', 'revenue_ksek': 15000, 'employees': 20,
         'year': 2023, 'size_bucket': 'small', 'industry': 'Finance', 'lat': 57.715678, 'lon': 11.945678},
    ])

    # Manufacturing companies
    companies.extend([
        {'name': 'SKF Sverige AB', 'orgnr': '5560789012', 'revenue_ksek': 500000, 'employees': 1000,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Manufacturing', 'lat': 57.721234, 'lon': 11.890123},
        {'name': 'Göteborg Mekaniska', 'orgnr': '5560890123', 'revenue_ksek': 45000, 'employees': 100,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Manufacturing', 'lat': 57.735678, 'lon': 11.912345},
        {'name': 'Precision Tools AB', 'orgnr': '5560901234', 'revenue_ksek': 30000, 'employees': 60,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Manufacturing', 'lat': 57.745678, 'lon': 11.923456},
    ])

    # Retail companies
    companies.extend([
        {'name': 'ICA Maxi Göteborg', 'orgnr': '5561012345', 'revenue_ksek': 120000, 'employees': 150,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Retail', 'lat': 57.689012, 'lon': 11.934567},
        {'name': 'Systembolaget City', 'orgnr': '5561123456', 'revenue_ksek': 80000, 'employees': 50,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Retail', 'lat': 57.705678, 'lon': 11.967890},
        {'name': 'Sportbutiken AB', 'orgnr': '5561234567', 'revenue_ksek': 18000, 'employees': 25,
         'year': 2023, 'size_bucket': 'small', 'industry': 'Retail', 'lat': 57.698765, 'lon': 11.956789},
    ])

    # Healthcare companies
    companies.extend([
        {'name': 'Sahlgrenska Life', 'orgnr': '5561345678', 'revenue_ksek': 250000, 'employees': 400,
         'year': 2023, 'size_bucket': 'large', 'industry': 'Healthcare', 'lat': 57.683456, 'lon': 11.962345},
        {'name': 'Vårdcentral Väst', 'orgnr': '5561456789', 'revenue_ksek': 35000, 'employees': 45,
         'year': 2023, 'size_bucket': 'medium', 'industry': 'Healthcare', 'lat': 57.654321, 'lon': 11.923456},
        {'name': 'Hälsokliniken', 'orgnr': '5561567890', 'revenue_ksek': 12000, 'employees': 15,
         'year': 2023, 'size_bucket': 'small', 'industry': 'Healthcare', 'lat': 57.712345, 'lon': 11.989012},
    ])

    # Add more companies spread around Gothenburg
    import random
    for i in range(20):
        lat = 57.65 + random.uniform(0, 0.1)
        lon = 11.9 + random.uniform(0, 0.15)
        revenue = random.randint(5000, 100000)

        size = 'small' if revenue < 20000 else 'medium' if revenue < 100000 else 'large'

        companies.append({
            'name': f'Local Business {i + 1} AB',
            'orgnr': f'556{200 + i:03d}{1000 + i:04d}',  # Format: 10 digits, no hyphen
            'revenue_ksek': revenue,
            'employees': random.randint(5, 200),
            'year': 2023,
            'size_bucket': size,
            'industry': random.choice(['Technology', 'Finance', 'Manufacturing', 'Retail', 'Healthcare']),
            'lat': lat,
            'lon': lon
        })

    df = pd.DataFrame(companies)

    with engine.begin() as conn:
        # Clear existing data
        conn.execute(text("DELETE FROM companies"))

        # Insert new data
        df.to_sql('companies', conn, if_exists='append', index=False)

    logger.info(f"Loaded {len(df)} companies")


def verify_data(engine):
    """Verify the loaded data."""
    with engine.connect() as conn:
        # Count associations
        assoc_count = conn.execute(text("SELECT COUNT(*) FROM associations")).scalar()
        logger.info(f"Total associations: {assoc_count}")

        # Count by size bucket
        size_counts = conn.execute(text("""
            SELECT size_bucket, COUNT(*) as count 
            FROM associations 
            GROUP BY size_bucket
        """)).fetchall()

        for size, count in size_counts:
            logger.info(f"  {size}: {count} associations")

        # Count companies
        comp_count = conn.execute(text("SELECT COUNT(*) FROM companies")).scalar()
        logger.info(f"Total companies: {comp_count}")

        # Count by industry
        industry_counts = conn.execute(text("""
            SELECT industry, COUNT(*) as count 
            FROM companies 
            GROUP BY industry
        """)).fetchall()

        for industry, count in industry_counts:
            logger.info(f"  {industry}: {count} companies")


if __name__ == "__main__":
    setup_database()


================================================================================
FIL: utils/train_clustering_models.py
================================================================================

#!/usr/bin/env python3
"""
train_clustering_models.py - Train clustering models with consistent features
"""

import logging
from pathlib import Path

import numpy as np
import pandas as pd
import joblib
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sqlalchemy import text

from sponsor_match.core.db import get_engine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ensure models directory exists
MODELS_DIR = Path(__file__).parent.parent / "models"
MODELS_DIR.mkdir(exist_ok=True)

# Feature configuration
FEATURES = ['lat', 'lon']  # Use only lat/lon for consistent features
N_CLUSTERS = {
    'default': 5,
    'large': 3
}


def load_data():
    """Load associations and companies from database."""
    engine = get_engine()

    with engine.connect() as conn:
        # Load associations
        associations = pd.read_sql("""
            SELECT id, name, lat, lon, size_bucket, member_count
            FROM associations
            WHERE lat IS NOT NULL AND lon IS NOT NULL
        """, conn)

        # Load companies
        companies = pd.read_sql("""
            SELECT id, name, lat, lon, size_bucket, revenue_ksek
            FROM companies
            WHERE lat IS NOT NULL AND lon IS NOT NULL
        """, conn)

    return associations, companies


def train_clustering_model(data, n_clusters, model_name):
    """Train a clustering model on the given data."""
    # Extract features
    X = data[FEATURES].values

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Train KMeans
    kmeans = KMeans(n_clusters=min(n_clusters, len(data)), random_state=42)
    kmeans.fit(X_scaled)

    # Create model package with scaler
    model_package = {
        'scaler': scaler,
        'kmeans': kmeans,
        'features': FEATURES,
        'n_features': len(FEATURES)
    }

    # Save model
    model_path = MODELS_DIR / f"{model_name}.joblib"
    joblib.dump(model_package, model_path)
    logger.info(f"Saved {model_name} model to {model_path}")

    # Report cluster sizes
    labels = kmeans.labels_
    unique, counts = np.unique(labels, return_counts=True)
    for label, count in zip(unique, counts):
        logger.info(f"  Cluster {label}: {count} items")

    return model_package


def train_all_models():
    """Train all clustering models."""
    # Load data
    associations, companies = load_data()

    if associations.empty or companies.empty:
        logger.error("No data found. Run setup_database.py first.")
        return

    logger.info(f"Loaded {len(associations)} associations and {len(companies)} companies")

    # Combine all data for default model
    all_data = pd.concat([
        associations[FEATURES + ['size_bucket']],
        companies[FEATURES + ['size_bucket']]
    ], ignore_index=True)

    # Train default model (for small/medium entities)
    default_data = all_data[all_data['size_bucket'].isin(['small', 'medium'])]
    if len(default_data) > 0:
        logger.info(f"\nTraining default model with {len(default_data)} entities")
        train_clustering_model(default_data, N_CLUSTERS['default'], 'kmeans')

    # Train large model (for large entities)
    large_data = all_data[all_data['size_bucket'] == 'large']
    if len(large_data) > 0:
        logger.info(f"\nTraining large model with {len(large_data)} entities")
        train_clustering_model(large_data, N_CLUSTERS['large'], 'kmeans_large')

    # Create simplified models for backward compatibility
    create_backward_compatible_models()


def create_backward_compatible_models():
    """Create simplified models for backward compatibility."""
    # Load the model packages
    default_package = joblib.load(MODELS_DIR / "kmeans.joblib")
    large_package = joblib.load(MODELS_DIR / "kmeans_large.joblib")

    # Extract just the KMeans models
    joblib.dump(default_package['kmeans'], MODELS_DIR / "kmeans_simple.joblib")
    joblib.dump(large_package['kmeans'], MODELS_DIR / "kmeans_large_simple.joblib")

    logger.info("Created backward compatible models")


def test_models():
    """Test the trained models."""
    # Load a model
    model_package = joblib.load(MODELS_DIR / "kmeans.joblib")
    scaler = model_package['scaler']
    kmeans = model_package['kmeans']

    # Test points in Gothenburg area
    test_points = [
        [57.7089, 11.9746],  # Central Gothenburg
        [57.6523, 11.9118],  # Västra Frölunda
        [57.7654, 11.9457],  # Hisings Kärra
    ]

    logger.info("\nTesting model predictions:")
    for point in test_points:
        scaled_point = scaler.transform([point])
        cluster = kmeans.predict(scaled_point)[0]
        logger.info(f"  Point {point} -> Cluster {cluster}")


if __name__ == "__main__":
    train_all_models()
    test_models()


================================================================================
FIL: .pytest_cache/README.md
================================================================================

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


================================================================================
FIL: data/examine.py
================================================================================

# examine.py - corrected version
import pandas as pd

# Since we're already in the data directory, just use the filename
df = pd.read_csv('companies_geocoded.csv')

# Filter to failed geocoding
failed = df[df['geocoding_confidence'] == 'failed']

# Look for patterns
print("Common patterns in failed addresses:")
print(failed['registered_address'].value_counts().head(20))

# Let's also understand more about these failures
print(f"\nTotal failed addresses: {len(failed)}")
print(f"Percentage of total: {len(failed) / len(df) * 100:.1f}%")

# Check if certain districts have more failures
print("\nFailed geocoding by district:")
district_failures = failed['district'].value_counts()
print(district_failures.head(10))

# Look for common words in failed addresses
print("\nCommon terms in failed addresses:")
common_terms = {}
for address in failed['registered_address'].dropna():
    words = address.lower().split()
    for word in words:
        if len(word) > 3:  # Skip short words
            common_terms[word] = common_terms.get(word, 0) + 1

# Sort by frequency
sorted_terms = sorted(common_terms.items(), key=lambda x: x[1], reverse=True)
for term, count in sorted_terms[:15]:
    print(f"  '{term}': {count} occurrences")

